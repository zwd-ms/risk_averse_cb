{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fc1416",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "* [kingcountyprices.arff](https://www.openml.org/search?type=data&status=active&id=42092)\n",
    "* [perthhouseprices.arff](https://www.openml.org/search?type=data&status=active&id=43822)\n",
    "* [diamondprices.arff](https://www.openml.org/search?type=data&status=active&id=42225)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2090e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353096ec",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "c5f963ff",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def flass(wut):\n",
    "    import arff\n",
    "    import pandas as pd\n",
    "    data = arff.load(open(wut, 'r'))\n",
    "    z = pd.DataFrame(data['data'])\n",
    "    z.columns = [ v[0].lower() for v in data['attributes'] ]\n",
    "    print(len(z[z['price'] < 1e6].index), len(z))\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "3208a67b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53940 53940\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>carat</th>\n",
       "      <th>cut</th>\n",
       "      <th>color</th>\n",
       "      <th>clarity</th>\n",
       "      <th>depth</th>\n",
       "      <th>table</th>\n",
       "      <th>price</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "      <th>z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>E</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.5</td>\n",
       "      <td>55.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.95</td>\n",
       "      <td>3.98</td>\n",
       "      <td>2.43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>Premium</td>\n",
       "      <td>E</td>\n",
       "      <td>SI1</td>\n",
       "      <td>59.8</td>\n",
       "      <td>61.0</td>\n",
       "      <td>326</td>\n",
       "      <td>3.89</td>\n",
       "      <td>3.84</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.23</td>\n",
       "      <td>Good</td>\n",
       "      <td>E</td>\n",
       "      <td>VS1</td>\n",
       "      <td>56.9</td>\n",
       "      <td>65.0</td>\n",
       "      <td>327</td>\n",
       "      <td>4.05</td>\n",
       "      <td>4.07</td>\n",
       "      <td>2.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.29</td>\n",
       "      <td>Premium</td>\n",
       "      <td>I</td>\n",
       "      <td>VS2</td>\n",
       "      <td>62.4</td>\n",
       "      <td>58.0</td>\n",
       "      <td>334</td>\n",
       "      <td>4.20</td>\n",
       "      <td>4.23</td>\n",
       "      <td>2.63</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.31</td>\n",
       "      <td>Good</td>\n",
       "      <td>J</td>\n",
       "      <td>SI2</td>\n",
       "      <td>63.3</td>\n",
       "      <td>58.0</td>\n",
       "      <td>335</td>\n",
       "      <td>4.34</td>\n",
       "      <td>4.35</td>\n",
       "      <td>2.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53935</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>60.8</td>\n",
       "      <td>57.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.75</td>\n",
       "      <td>5.76</td>\n",
       "      <td>3.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53936</th>\n",
       "      <td>0.72</td>\n",
       "      <td>Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>63.1</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.69</td>\n",
       "      <td>5.75</td>\n",
       "      <td>3.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53937</th>\n",
       "      <td>0.70</td>\n",
       "      <td>Very Good</td>\n",
       "      <td>D</td>\n",
       "      <td>SI1</td>\n",
       "      <td>62.8</td>\n",
       "      <td>60.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.66</td>\n",
       "      <td>5.68</td>\n",
       "      <td>3.56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53938</th>\n",
       "      <td>0.86</td>\n",
       "      <td>Premium</td>\n",
       "      <td>H</td>\n",
       "      <td>SI2</td>\n",
       "      <td>61.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>6.15</td>\n",
       "      <td>6.12</td>\n",
       "      <td>3.74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53939</th>\n",
       "      <td>0.75</td>\n",
       "      <td>Ideal</td>\n",
       "      <td>D</td>\n",
       "      <td>SI2</td>\n",
       "      <td>62.2</td>\n",
       "      <td>55.0</td>\n",
       "      <td>2757</td>\n",
       "      <td>5.83</td>\n",
       "      <td>5.87</td>\n",
       "      <td>3.64</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>53940 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       carat        cut color clarity  depth  table  price     x     y     z\n",
       "0       0.23      Ideal     E     SI2   61.5   55.0    326  3.95  3.98  2.43\n",
       "1       0.21    Premium     E     SI1   59.8   61.0    326  3.89  3.84  2.31\n",
       "2       0.23       Good     E     VS1   56.9   65.0    327  4.05  4.07  2.31\n",
       "3       0.29    Premium     I     VS2   62.4   58.0    334  4.20  4.23  2.63\n",
       "4       0.31       Good     J     SI2   63.3   58.0    335  4.34  4.35  2.75\n",
       "...      ...        ...   ...     ...    ...    ...    ...   ...   ...   ...\n",
       "53935   0.72      Ideal     D     SI1   60.8   57.0   2757  5.75  5.76  3.50\n",
       "53936   0.72       Good     D     SI1   63.1   55.0   2757  5.69  5.75  3.61\n",
       "53937   0.70  Very Good     D     SI1   62.8   60.0   2757  5.66  5.68  3.56\n",
       "53938   0.86    Premium     H     SI2   61.0   58.0   2757  6.15  6.12  3.74\n",
       "53939   0.75      Ideal     D     SI2   62.2   55.0   2757  5.83  5.87  3.64\n",
       "\n",
       "[53940 rows x 10 columns]"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flass('diamondprices.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "481cfe77",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29422 33656\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>address</th>\n",
       "      <th>suburb</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>garage</th>\n",
       "      <th>land_area</th>\n",
       "      <th>floor_area</th>\n",
       "      <th>build_year</th>\n",
       "      <th>cbd_dist</th>\n",
       "      <th>nearest_stn</th>\n",
       "      <th>nearest_stn_dist</th>\n",
       "      <th>date_sold</th>\n",
       "      <th>postcode</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>nearest_sch</th>\n",
       "      <th>nearest_sch_dist</th>\n",
       "      <th>nearest_sch_rank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1 Acorn Place</td>\n",
       "      <td>South Lake</td>\n",
       "      <td>565000</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>600</td>\n",
       "      <td>160</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>18300</td>\n",
       "      <td>Cockburn Central Station</td>\n",
       "      <td>1800</td>\n",
       "      <td>09-2018</td>\n",
       "      <td>6164</td>\n",
       "      <td>-32.115900</td>\n",
       "      <td>115.842450</td>\n",
       "      <td>LAKELAND SENIOR HIGH SCHOOL</td>\n",
       "      <td>0.828339</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1 Addis Way</td>\n",
       "      <td>Wandi</td>\n",
       "      <td>365000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>351</td>\n",
       "      <td>139</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>26900</td>\n",
       "      <td>Kwinana Station</td>\n",
       "      <td>4900</td>\n",
       "      <td>02-2019</td>\n",
       "      <td>6167</td>\n",
       "      <td>-32.193470</td>\n",
       "      <td>115.859554</td>\n",
       "      <td>ATWELL COLLEGE</td>\n",
       "      <td>5.524324</td>\n",
       "      <td>129.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1 Ainsley Court</td>\n",
       "      <td>Camillo</td>\n",
       "      <td>287000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>719</td>\n",
       "      <td>86</td>\n",
       "      <td>1979.0</td>\n",
       "      <td>22600</td>\n",
       "      <td>Challis Station</td>\n",
       "      <td>1900</td>\n",
       "      <td>06-2015</td>\n",
       "      <td>6111</td>\n",
       "      <td>-32.120578</td>\n",
       "      <td>115.993579</td>\n",
       "      <td>KELMSCOTT SENIOR HIGH SCHOOL</td>\n",
       "      <td>1.649178</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1 Albert Street</td>\n",
       "      <td>Bellevue</td>\n",
       "      <td>255000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>651</td>\n",
       "      <td>59</td>\n",
       "      <td>1953.0</td>\n",
       "      <td>17900</td>\n",
       "      <td>Midland Station</td>\n",
       "      <td>3600</td>\n",
       "      <td>07-2018</td>\n",
       "      <td>6056</td>\n",
       "      <td>-31.900547</td>\n",
       "      <td>116.038009</td>\n",
       "      <td>SWAN VIEW SENIOR HIGH SCHOOL</td>\n",
       "      <td>1.571401</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1 Aman Place</td>\n",
       "      <td>Lockridge</td>\n",
       "      <td>325000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>466</td>\n",
       "      <td>131</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>11200</td>\n",
       "      <td>Bassendean Station</td>\n",
       "      <td>2000</td>\n",
       "      <td>11-2016</td>\n",
       "      <td>6054</td>\n",
       "      <td>-31.885790</td>\n",
       "      <td>115.947780</td>\n",
       "      <td>KIARA COLLEGE</td>\n",
       "      <td>1.514922</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33651</th>\n",
       "      <td>9C Gold Street</td>\n",
       "      <td>South Fremantle</td>\n",
       "      <td>1040000</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>292</td>\n",
       "      <td>245</td>\n",
       "      <td>2013.0</td>\n",
       "      <td>16100</td>\n",
       "      <td>Fremantle Station</td>\n",
       "      <td>1500</td>\n",
       "      <td>03-2016</td>\n",
       "      <td>6162</td>\n",
       "      <td>-32.064580</td>\n",
       "      <td>115.751820</td>\n",
       "      <td>CHRISTIAN BROTHERS' COLLEGE</td>\n",
       "      <td>1.430350</td>\n",
       "      <td>49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33652</th>\n",
       "      <td>9C Pycombe Way</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>410000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>228</td>\n",
       "      <td>114</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9600</td>\n",
       "      <td>Stirling Station</td>\n",
       "      <td>4600</td>\n",
       "      <td>02-2017</td>\n",
       "      <td>6061</td>\n",
       "      <td>-31.867055</td>\n",
       "      <td>115.841403</td>\n",
       "      <td>JOHN SEPTIMUS ROE ANGLICAN COMMUNITY SCHOOL</td>\n",
       "      <td>1.679644</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33653</th>\n",
       "      <td>9D Pycombe Way</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>427000</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>261</td>\n",
       "      <td>112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9600</td>\n",
       "      <td>Stirling Station</td>\n",
       "      <td>4600</td>\n",
       "      <td>02-2017</td>\n",
       "      <td>6061</td>\n",
       "      <td>-31.866890</td>\n",
       "      <td>115.841418</td>\n",
       "      <td>JOHN SEPTIMUS ROE ANGLICAN COMMUNITY SCHOOL</td>\n",
       "      <td>1.669159</td>\n",
       "      <td>35.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33654</th>\n",
       "      <td>9D Shalford Way</td>\n",
       "      <td>Girrawheen</td>\n",
       "      <td>295000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>457</td>\n",
       "      <td>85</td>\n",
       "      <td>1974.0</td>\n",
       "      <td>12600</td>\n",
       "      <td>Warwick Station</td>\n",
       "      <td>4400</td>\n",
       "      <td>10-2016</td>\n",
       "      <td>6064</td>\n",
       "      <td>-31.839680</td>\n",
       "      <td>115.842410</td>\n",
       "      <td>GIRRAWHEEN SENIOR HIGH SCHOOL</td>\n",
       "      <td>0.358494</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33655</th>\n",
       "      <td>9E Margaret Street</td>\n",
       "      <td>Midland</td>\n",
       "      <td>295000</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>296</td>\n",
       "      <td>95</td>\n",
       "      <td>NaN</td>\n",
       "      <td>16700</td>\n",
       "      <td>Midland Station</td>\n",
       "      <td>1700</td>\n",
       "      <td>05-2016</td>\n",
       "      <td>6056</td>\n",
       "      <td>-31.882163</td>\n",
       "      <td>116.014755</td>\n",
       "      <td>LA SALLE COLLEGE</td>\n",
       "      <td>1.055564</td>\n",
       "      <td>53.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>33656 rows Ã— 19 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  address           suburb    price  bedrooms  bathrooms  \\\n",
       "0           1 Acorn Place       South Lake   565000         4          2   \n",
       "1             1 Addis Way            Wandi   365000         3          2   \n",
       "2         1 Ainsley Court          Camillo   287000         3          1   \n",
       "3         1 Albert Street         Bellevue   255000         2          1   \n",
       "4            1 Aman Place        Lockridge   325000         4          1   \n",
       "...                   ...              ...      ...       ...        ...   \n",
       "33651      9C Gold Street  South Fremantle  1040000         4          3   \n",
       "33652      9C Pycombe Way      Westminster   410000         3          2   \n",
       "33653      9D Pycombe Way      Westminster   427000         3          2   \n",
       "33654     9D Shalford Way       Girrawheen   295000         3          1   \n",
       "33655  9E Margaret Street          Midland   295000         3          1   \n",
       "\n",
       "       garage  land_area  floor_area  build_year  cbd_dist  \\\n",
       "0         2.0        600         160      2003.0     18300   \n",
       "1         2.0        351         139      2013.0     26900   \n",
       "2         1.0        719          86      1979.0     22600   \n",
       "3         2.0        651          59      1953.0     17900   \n",
       "4         2.0        466         131      1998.0     11200   \n",
       "...       ...        ...         ...         ...       ...   \n",
       "33651     2.0        292         245      2013.0     16100   \n",
       "33652     2.0        228         114         NaN      9600   \n",
       "33653     2.0        261         112         NaN      9600   \n",
       "33654     2.0        457          85      1974.0     12600   \n",
       "33655     2.0        296          95         NaN     16700   \n",
       "\n",
       "                    nearest_stn  nearest_stn_dist date_sold  postcode  \\\n",
       "0      Cockburn Central Station              1800   09-2018      6164   \n",
       "1               Kwinana Station              4900   02-2019      6167   \n",
       "2               Challis Station              1900   06-2015      6111   \n",
       "3               Midland Station              3600   07-2018      6056   \n",
       "4            Bassendean Station              2000   11-2016      6054   \n",
       "...                         ...               ...       ...       ...   \n",
       "33651         Fremantle Station              1500   03-2016      6162   \n",
       "33652          Stirling Station              4600   02-2017      6061   \n",
       "33653          Stirling Station              4600   02-2017      6061   \n",
       "33654           Warwick Station              4400   10-2016      6064   \n",
       "33655           Midland Station              1700   05-2016      6056   \n",
       "\n",
       "        latitude   longitude                                  nearest_sch  \\\n",
       "0     -32.115900  115.842450                  LAKELAND SENIOR HIGH SCHOOL   \n",
       "1     -32.193470  115.859554                               ATWELL COLLEGE   \n",
       "2     -32.120578  115.993579                 KELMSCOTT SENIOR HIGH SCHOOL   \n",
       "3     -31.900547  116.038009                 SWAN VIEW SENIOR HIGH SCHOOL   \n",
       "4     -31.885790  115.947780                                KIARA COLLEGE   \n",
       "...          ...         ...                                          ...   \n",
       "33651 -32.064580  115.751820                  CHRISTIAN BROTHERS' COLLEGE   \n",
       "33652 -31.867055  115.841403  JOHN SEPTIMUS ROE ANGLICAN COMMUNITY SCHOOL   \n",
       "33653 -31.866890  115.841418  JOHN SEPTIMUS ROE ANGLICAN COMMUNITY SCHOOL   \n",
       "33654 -31.839680  115.842410                GIRRAWHEEN SENIOR HIGH SCHOOL   \n",
       "33655 -31.882163  116.014755                             LA SALLE COLLEGE   \n",
       "\n",
       "       nearest_sch_dist  nearest_sch_rank  \n",
       "0              0.828339               NaN  \n",
       "1              5.524324             129.0  \n",
       "2              1.649178             113.0  \n",
       "3              1.571401               NaN  \n",
       "4              1.514922               NaN  \n",
       "...                 ...               ...  \n",
       "33651          1.430350              49.0  \n",
       "33652          1.679644              35.0  \n",
       "33653          1.669159              35.0  \n",
       "33654          0.358494               NaN  \n",
       "33655          1.055564              53.0  \n",
       "\n",
       "[33656 rows x 19 columns]"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flass('perthhouseprices.arff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "297324ad",
   "metadata": {
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20121 21613\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7129300520</td>\n",
       "      <td>20141013T000000</td>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98178</td>\n",
       "      <td>47.5112</td>\n",
       "      <td>-122.257</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6414100192</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1951</td>\n",
       "      <td>1991</td>\n",
       "      <td>98125</td>\n",
       "      <td>47.7210</td>\n",
       "      <td>-122.319</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5631500400</td>\n",
       "      <td>20150225T000000</td>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>1933</td>\n",
       "      <td>0</td>\n",
       "      <td>98028</td>\n",
       "      <td>47.7379</td>\n",
       "      <td>-122.233</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2487200875</td>\n",
       "      <td>20141209T000000</td>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1965</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5208</td>\n",
       "      <td>-122.393</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1954400510</td>\n",
       "      <td>20150218T000000</td>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1987</td>\n",
       "      <td>0</td>\n",
       "      <td>98074</td>\n",
       "      <td>47.6168</td>\n",
       "      <td>-122.045</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>263000018</td>\n",
       "      <td>20140521T000000</td>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98103</td>\n",
       "      <td>47.6993</td>\n",
       "      <td>-122.346</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>6600060120</td>\n",
       "      <td>20150223T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>2014</td>\n",
       "      <td>0</td>\n",
       "      <td>98146</td>\n",
       "      <td>47.5107</td>\n",
       "      <td>-122.362</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>1523300141</td>\n",
       "      <td>20140623T000000</td>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2009</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5944</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>291310100</td>\n",
       "      <td>20150116T000000</td>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>2004</td>\n",
       "      <td>0</td>\n",
       "      <td>98027</td>\n",
       "      <td>47.5345</td>\n",
       "      <td>-122.069</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>1523300157</td>\n",
       "      <td>20141015T000000</td>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>2008</td>\n",
       "      <td>0</td>\n",
       "      <td>98144</td>\n",
       "      <td>47.5941</td>\n",
       "      <td>-122.299</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  bedrooms  bathrooms  \\\n",
       "0      7129300520  20141013T000000  221900.0         3       1.00   \n",
       "1      6414100192  20141209T000000  538000.0         3       2.25   \n",
       "2      5631500400  20150225T000000  180000.0         2       1.00   \n",
       "3      2487200875  20141209T000000  604000.0         4       3.00   \n",
       "4      1954400510  20150218T000000  510000.0         3       2.00   \n",
       "...           ...              ...       ...       ...        ...   \n",
       "21608   263000018  20140521T000000  360000.0         3       2.50   \n",
       "21609  6600060120  20150223T000000  400000.0         4       2.50   \n",
       "21610  1523300141  20140623T000000  402101.0         2       0.75   \n",
       "21611   291310100  20150116T000000  400000.0         3       2.50   \n",
       "21612  1523300157  20141015T000000  325000.0         2       0.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "0             1180      5650     1.0           0     0  ...      7   \n",
       "1             2570      7242     2.0           0     0  ...      7   \n",
       "2              770     10000     1.0           0     0  ...      6   \n",
       "3             1960      5000     1.0           0     0  ...      7   \n",
       "4             1680      8080     1.0           0     0  ...      8   \n",
       "...            ...       ...     ...         ...   ...  ...    ...   \n",
       "21608         1530      1131     3.0           0     0  ...      8   \n",
       "21609         2310      5813     2.0           0     0  ...      8   \n",
       "21610         1020      1350     2.0           0     0  ...      7   \n",
       "21611         1600      2388     2.0           0     0  ...      8   \n",
       "21612         1020      1076     2.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "0            1180              0      1955             0    98178  47.5112   \n",
       "1            2170            400      1951          1991    98125  47.7210   \n",
       "2             770              0      1933             0    98028  47.7379   \n",
       "3            1050            910      1965             0    98136  47.5208   \n",
       "4            1680              0      1987             0    98074  47.6168   \n",
       "...           ...            ...       ...           ...      ...      ...   \n",
       "21608        1530              0      2009             0    98103  47.6993   \n",
       "21609        2310              0      2014             0    98146  47.5107   \n",
       "21610        1020              0      2009             0    98144  47.5944   \n",
       "21611        1600              0      2004             0    98027  47.5345   \n",
       "21612        1020              0      2008             0    98144  47.5941   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "0     -122.257           1340        5650  \n",
       "1     -122.319           1690        7639  \n",
       "2     -122.233           2720        8062  \n",
       "3     -122.393           1360        5000  \n",
       "4     -122.045           1800        7503  \n",
       "...        ...            ...         ...  \n",
       "21608 -122.346           1530        1509  \n",
       "21609 -122.362           1830        7200  \n",
       "21610 -122.299           1020        2007  \n",
       "21611 -122.069           1410        1287  \n",
       "21612 -122.299           1020        1357  \n",
       "\n",
       "[21613 rows x 21 columns]"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flass('kingcountyprices.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4381a8",
   "metadata": {},
   "source": [
    "# Arff2Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e68df6d",
   "metadata": {
    "code_folding": [
     2,
     35,
     36,
     46,
     56,
     59,
     65,
     69,
     70,
     122,
     165
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EasyAcc:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.sum = 0\n",
    "        self.sumsq = 0\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        import math\n",
    "        if not math.isnan(other):\n",
    "            self.n += 1\n",
    "            self.sum += other\n",
    "            self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        import math\n",
    "        if not math.isnan(other):\n",
    "            self.n += 1\n",
    "            self.sum -= other\n",
    "            self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / max(self.n, 1)\n",
    "\n",
    "    def var(self):\n",
    "        from math import sqrt\n",
    "        return sqrt(self.sumsq / max(self.n, 1) - self.mean()**2)\n",
    "\n",
    "    def semean(self):\n",
    "        from math import sqrt\n",
    "        return self.var() / sqrt(max(self.n, 1))\n",
    "\n",
    "class EasyPoissonBootstrapAcc:\n",
    "    def __init__(self, batch_size, confidence=0.95, seed=2112):\n",
    "        from math import ceil\n",
    "        from numpy.random import default_rng\n",
    "        \n",
    "        self.n = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.confidence = confidence\n",
    "        self.samples = [ EasyAcc() for _ in range(int(ceil(3 / (1 - self.confidence)))) ]\n",
    "        self.rng = default_rng(seed)\n",
    "        \n",
    "    def __iadd__(self, other):\n",
    "        self.n += 1\n",
    "        \n",
    "        poissons = self.rng.poisson(lam=self.batch_size, size=len(self.samples)) / self.batch_size\n",
    "        \n",
    "        for n, (chirp, acc) in enumerate(zip(poissons, self.samples)):\n",
    "            acc += (chirp if n > 0 else 1) * other\n",
    "            \n",
    "        return self\n",
    "         \n",
    "    def __isub__(self, other):\n",
    "        return self.__iadd__(-other)\n",
    "    \n",
    "    def ci(self):\n",
    "        import numpy\n",
    "        quantiles = numpy.quantile(a=[ x.mean() for x in self.samples ],\n",
    "                                   q=[1 - self.confidence, 0.5, self.confidence])\n",
    "        return list(quantiles)\n",
    "    \n",
    "    def formatci(self):\n",
    "        z = self.ci()\n",
    "        return '[{:<.4f},{:<.4f}]'.format(z[0], z[2])\n",
    "    \n",
    "class Schema(object):\n",
    "    def __init__(self, *, attributes, target, skipcol, data):\n",
    "        super().__init__()\n",
    "        \n",
    "        schema = {}\n",
    "        n = 0\n",
    "        for kraw, v in attributes:\n",
    "            k = kraw.lower()\n",
    "\n",
    "            if k in skipcol:\n",
    "                continue\n",
    "                \n",
    "            if isinstance(v, str):\n",
    "                if v in ['INTEGER', 'REAL']:\n",
    "                    if any(thisv is None for row in data for thisk, thisv in zip(attributes, row) if thisk[0].lower() == k):\n",
    "                        assert k != target, (k, target)\n",
    "                        schema[k] = (lambda i: (lambda z: (i+1, 1) if z is None else (i, z)))(n)\n",
    "                        n += 2\n",
    "                    else:\n",
    "                        schema[k] = (lambda i: (lambda z: (i, z)))(n)\n",
    "                        n += 1\n",
    "                elif k == 'date':\n",
    "                    import ciso8601\n",
    "                    import time\n",
    "                    \n",
    "                    schema[k] = (lambda i: (lambda z: (i, time.mktime(ciso8601.parse_datetime(z).timetuple()))))(n)\n",
    "                    n += 1\n",
    "                elif v == 'STRING':\n",
    "                    uniques = set([ thisv for row in data for thisk, thisv in zip(attributes, row) if thisk[0].lower() == k ])\n",
    "                    schema[k] =  (lambda h: (lambda z: (h[z], 1)))({ z: (n + m) for m, z in enumerate(uniques) })\n",
    "                    n += len(uniques)\n",
    "                else:\n",
    "                    assert False, (k, v)\n",
    "            elif isinstance(v, list) and all((isinstance(z, str) for z in v)):\n",
    "                assert k != target, (k, target)\n",
    "                schema[k] = (lambda h: (lambda z: (h[z], 1)))({ z: (n + m) for m, z in enumerate(v) })\n",
    "                n += len(v)\n",
    "            else:\n",
    "                assert False\n",
    "                \n",
    "            if k == target:\n",
    "                n -= 1\n",
    "                \n",
    "        assert target in schema, (target, attributes)\n",
    "                \n",
    "        self.schema = schema\n",
    "        self.target = target\n",
    "        self.nfeatures = n \n",
    "        \n",
    "    def featurize(self, colname, val):\n",
    "        if colname in self.schema:\n",
    "            yield self.schema[colname](val)\n",
    "\n",
    "def makeData(filename, *, target, skipcol, skiprow):\n",
    "    import arff\n",
    "    import numpy\n",
    "    \n",
    "    data = arff.load(open(filename, 'r'))\n",
    "    schema = Schema(attributes=data['attributes'], target=target, skipcol=skipcol, data=data['data'])\n",
    "    \n",
    "    Y = []\n",
    "    X = []\n",
    "    \n",
    "    for row in data['data']:\n",
    "        hashrow = { kraw[0].lower(): v for kraw, v in zip(data['attributes'], row) }\n",
    "        \n",
    "        if skiprow(hashrow):\n",
    "            continue\n",
    "        \n",
    "        y = None\n",
    "        x = [0]*schema.nfeatures\n",
    "        for col, val in hashrow.items():\n",
    "            if col == target:\n",
    "                y = next(schema.featurize(col, val))[1]\n",
    "            else:\n",
    "                for f, vf in schema.featurize(col, val):\n",
    "                    from numbers import Number\n",
    "                    assert isinstance(vf, Number), (col, val, f, vf)\n",
    "                    x[f] = vf\n",
    "                    \n",
    "        Y.append(y)\n",
    "        X.append(x)\n",
    "\n",
    "    Y = numpy.array(Y)\n",
    "    Ymin, Ymax = numpy.min(Y), numpy.max(Y)\n",
    "    Y = (Y - Ymin) / (Ymax - Ymin)\n",
    "    X = numpy.array(X)\n",
    "    Xmin, Xmax = numpy.min(X, axis=0, keepdims=True), numpy.max(X, axis=0, keepdims=True)\n",
    "    if numpy.any(Xmin >= Xmax):\n",
    "        X = X[:,Xmin[0,:] < Xmax[0,:]]\n",
    "        Xmin, Xmax = numpy.min(X, axis=0, keepdims=True), numpy.max(X, axis=0, keepdims=True)\n",
    "    assert numpy.all(Xmax > Xmin), [ (col, lb, ub) for col, (lb, ub) in enumerate(zip(Xmin[0,:], Xmax[0,:])) if lb >= ub ]\n",
    "    X = (X - Xmin) / (Xmax - Xmin)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "class ArffToPytorch(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename, *, target, skipcol, skiprow):\n",
    "        X, Y = makeData(filename, target=target, skipcol=skipcol, skiprow=skiprow)\n",
    "        self.Xs = torch.Tensor(X)\n",
    "        self.Ys = torch.Tensor(Y).unsqueeze(1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.Xs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        return self.Xs[index], self.Ys[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48462203",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "89fdff52",
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_predict': 0.38810810446739197,\n",
       " 'best_constant_loss_predict': 0.17130790650844574,\n",
       " 'best_constant_average_reward': 0.8286920934915543,\n",
       " 'best_constant_average_logloss': 0.4579547441194928}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best constant predictor\n",
    "# if you don't beat this, you have a problem\n",
    "\n",
    "def bestconstant(dataset):\n",
    "    import numpy\n",
    "        \n",
    "    ymed = torch.median(dataset.Ys).item()\n",
    "    ymedabsloss = torch.mean(torch.abs(dataset.Ys - ymed)).item()\n",
    "\n",
    "    l1_loss = torch.nn.L1Loss(reduction='none')\n",
    "    log_loss = torch.nn.BCELoss()\n",
    "    sumabsloss, sumlogloss = EasyAcc(), EasyAcc()\n",
    "    \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for _, Ys in generator:\n",
    "        with torch.no_grad():\n",
    "            yhat = torch.Tensor([ymed]).expand(*Ys.shape)\n",
    "            loss = l1_loss(input=yhat, target=Ys)\n",
    "            losspredict = torch.Tensor([ymedabsloss]).expand(*Ys.shape)\n",
    "            loglosspredict = log_loss(input=losspredict, target=loss)\n",
    "            \n",
    "            sumabsloss += torch.mean(losspredict).item()\n",
    "            sumlogloss += torch.mean(loglosspredict).item()\n",
    "    \n",
    "    return { 'best_constant_predict': ymed,\n",
    "             'best_constant_loss_predict': ymedabsloss,\n",
    "             'best_constant_average_reward': 1 - sumabsloss.mean(), \n",
    "             'best_constant_average_logloss': sumlogloss.mean(),\n",
    "           }\n",
    "\n",
    "bestconstant(ArffToPytorch('kingcountyprices.arff', target='price', skipcol=['id'], skiprow=lambda z: z['price'] > 1e6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eca68a96",
   "metadata": {
    "code_folding": [
     0,
     1,
     10,
     11,
     25,
     31
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LinearArgmax(torch.nn.Module):\n",
    "    def __init__(self, dobs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(in_features=dobs, out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, Xs):\n",
    "        return self.sigmoid(self.linear(Xs))\n",
    "\n",
    "class CauchyRFF(torch.nn.Module):\n",
    "    def __init__(self, dobs, numrff, sigma, device):\n",
    "        from math import pi, sqrt\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rffW = torch.nn.Parameter(torch.empty(dobs, numrff).cauchy_(sigma = sigma).to(device), \n",
    "                                       requires_grad=False)\n",
    "        self.rffb = torch.nn.Parameter((2 * pi * torch.rand(numrff)).to(device),\n",
    "                                       requires_grad=False)\n",
    "        self.sqrtrff = torch.nn.Parameter(torch.Tensor([sqrt(numrff)]).to(device), \n",
    "                                          requires_grad=False)\n",
    "        self.linear = torch.nn.Linear(in_features=numrff, out_features=1, device=device)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, Xs):\n",
    "        with torch.no_grad():\n",
    "            rff = (torch.matmul(Xs, self.rffW) + self.rffb).cos() / self.sqrtrff\n",
    "            \n",
    "        return self.sigmoid(self.linear(rff))\n",
    "    \n",
    "def learnOnline(dataset, *, seed, batch_size, initlr, tzero, modelfactory):\n",
    "    import time\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = None\n",
    "    l1_loss = torch.nn.L1Loss()\n",
    "    \n",
    "    print('{:<5s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}'.format('n', 'loss', 'since last', 'acc', 'acc since last', 'dt (sec)'),\n",
    "          flush=True)\n",
    "    avloss, acc, sincelast, accsincelast = EasyAcc(), EasyAcc(), EasyAcc(), EasyAcc()\n",
    "    \n",
    "    for bno, (Xs, ys) in enumerate(generator):\n",
    "        if model is None:\n",
    "            from math import sqrt\n",
    "            model = modelfactory(Xs.shape[1])\n",
    "            opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: sqrt(tzero) / sqrt(tzero + t))\n",
    "            start = time.time()\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        yhat = model(Xs)\n",
    "        loss = l1_loss(yhat, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc += torch.mean((torch.abs(yhat - ys) <= 0.1).float())\n",
    "            accsincelast += torch.mean((torch.abs(yhat - ys) <= 0.1).float())\n",
    "            avloss += loss\n",
    "            sincelast += loss\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            print('{:<5d}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}'.format(\n",
    "                    avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "                  flush=True)\n",
    "            sincelast, accsincelast = EasyAcc(), EasyAcc()\n",
    "\n",
    "    print('{:<5d}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}'.format(\n",
    "            avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4d1196ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mydata = ArffToPytorch('kingcountyprices.arff', target='price', skipcol=['id'], skiprow=lambda z: z['price'] > 1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aaa22be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.20138 \t0.20138 \t0.00000 \t0.00000 \t0.00196 \n",
      "2    \t0.25316 \t0.30494 \t0.00000 \t0.00000 \t0.00562 \n",
      "3    \t0.26323 \t0.28336 \t0.00000 \t0.00000 \t0.00861 \n",
      "5    \t0.19947 \t0.10385 \t0.20000 \t0.50000 \t0.01556 \n",
      "9    \t0.24191 \t0.29495 \t0.11111 \t0.00000 \t0.02436 \n",
      "17   \t0.20296 \t0.15913 \t0.23529 \t0.37500 \t0.03347 \n",
      "33   \t0.19764 \t0.19200 \t0.27273 \t0.31250 \t0.04634 \n",
      "65   \t0.18754 \t0.17712 \t0.29231 \t0.31250 \t0.07089 \n",
      "129  \t0.17111 \t0.15443 \t0.32558 \t0.35938 \t0.11636 \n",
      "257  \t0.17076 \t0.17041 \t0.30350 \t0.28125 \t0.20707 \n",
      "513  \t0.15818 \t0.14554 \t0.35867 \t0.41406 \t0.34772 \n",
      "1025 \t0.14074 \t0.12326 \t0.43707 \t0.51562 \t0.60899 \n",
      "2049 \t0.12219 \t0.10363 \t0.51049 \t0.58398 \t1.18835 \n",
      "4097 \t0.10565 \t0.08910 \t0.58189 \t0.65332 \t2.18929 \n",
      "8193 \t0.09126 \t0.07688 \t0.65019 \t0.71851 \t4.24810 \n",
      "16385\t0.08129 \t0.07132 \t0.70400 \t0.75781 \t8.26236 \n",
      "20148\t0.07877 \t0.06777 \t0.71551 \t0.76561 \t10.13706\n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, seed=4545, batch_size=1, initlr=1e-2, tzero=100, modelfactory=lambda x: LinearArgmax(dobs=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d98f34f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.15748 \t0.15748 \t0.00000 \t0.00000 \t0.00122 \n",
      "2    \t0.21317 \t0.26885 \t0.00000 \t0.00000 \t0.00371 \n",
      "3    \t0.20577 \t0.19098 \t0.00000 \t0.00000 \t0.00605 \n",
      "5    \t0.16110 \t0.09409 \t0.20000 \t0.50000 \t0.00903 \n",
      "9    \t0.16096 \t0.16079 \t0.22222 \t0.25000 \t0.01284 \n",
      "17   \t0.19002 \t0.22271 \t0.23529 \t0.25000 \t0.02552 \n",
      "33   \t0.17721 \t0.16360 \t0.39394 \t0.56250 \t0.04771 \n",
      "65   \t0.17054 \t0.16365 \t0.36923 \t0.34375 \t0.07848 \n",
      "129  \t0.14692 \t0.12294 \t0.41860 \t0.46875 \t0.13561 \n",
      "257  \t0.13691 \t0.12682 \t0.45136 \t0.48438 \t0.25272 \n",
      "513  \t0.12149 \t0.10602 \t0.53996 \t0.62891 \t0.47650 \n",
      "1025 \t0.10400 \t0.08646 \t0.61951 \t0.69922 \t0.84510 \n",
      "2049 \t0.09017 \t0.07634 \t0.67448 \t0.72949 \t1.57256 \n",
      "4097 \t0.08170 \t0.07322 \t0.71223 \t0.75000 \t3.21442 \n",
      "8193 \t0.07422 \t0.06674 \t0.74710 \t0.78198 \t6.45025 \n",
      "16385\t0.06916 \t0.06411 \t0.77443 \t0.80176 \t12.09390\n",
      "20148\t0.06773 \t0.06146 \t0.78186 \t0.81424 \t14.64487\n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, seed=4545, batch_size=1, initlr=1e-2, tzero=100, \n",
    "            modelfactory=lambda x: CauchyRFF(dobs=x, numrff=1024, sigma=1/10, device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bf77d",
   "metadata": {},
   "source": [
    "# Bandit Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860526c2",
   "metadata": {},
   "source": [
    "| dataset | $q$ | Argmin | Avg. Commission | No sale (%) | \n",
    "| --- | --- | --- | --- | --- |\n",
    "| king county | 0.2 | approx | [26.23,26.82] | [14.31,14.68] |\n",
    "| king county | 0.2 | exact | [26.07,26.63] | [18.09,18.49] |\n",
    "| king county | 0.5 | approx | [27.89,28.48] | [17.52,18.02] |\n",
    "| king county | 0.5 | exact | [27.29,27.88] | [19.75,20.32] |\n",
    "| perth | 0.2 | approx | [31.37,31.84] | [11.48,11.75] |\n",
    "| perth | 0.2 | exact | [31.21,31.67] | [12.94,13.23] |\n",
    "| perth | 0.5 | approx | [31.71,32.19]\t | [21.71,22.15] |\n",
    "| perth | 0.5 | exact | [31.13,31.61] | [22.52,22.98] |\n",
    "| diamonds | 0.2 | approx | ... | ... |\n",
    "| diamonds | 0.2 | exact | ... | ... |\n",
    "| diamonds | 0.5 | approx | ... | ... |\n",
    "| diamonds | 0.5 | approx | ... | ... |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bb5bb1d8",
   "metadata": {
    "code_folding": [
     0,
     1,
     17,
     24,
     30,
     41,
     56,
     57,
     66,
     92,
     113,
     130,
     141,
     155
    ]
   },
   "outputs": [],
   "source": [
    "class CauchyTruncatedNormal(torch.nn.Module):\n",
    "    def __init__(self, dobs, numrff, sigma, device, approxargmax):\n",
    "        from math import pi, sqrt\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rffW = torch.nn.Parameter(torch.empty(dobs, numrff).cauchy_(sigma = sigma).to(device), \n",
    "                                       requires_grad=False)\n",
    "        self.rffb = torch.nn.Parameter((2 * pi * torch.rand(numrff)).to(device),\n",
    "                                       requires_grad=False)\n",
    "        self.sqrtrff = torch.nn.Parameter(torch.Tensor([sqrt(numrff)]).to(device), \n",
    "                                          requires_grad=False)\n",
    "        self.linear = torch.nn.Linear(in_features=numrff, out_features=2, device=device)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.approxargmax = approxargmax\n",
    "        \n",
    "    def distparams(self, Xs):\n",
    "        with torch.no_grad():\n",
    "            rff = (torch.matmul(Xs, self.rffW) + self.rffb).cos() / self.sqrtrff\n",
    "            \n",
    "        pre = self.linear(rff)\n",
    "        return self.sigmoid(pre[:,0:1]), 0.01 + self.softplus(pre[:,1:2])\n",
    "    \n",
    "    def forward(self, Xs, As):\n",
    "        mu, sigma = self.distparams(Xs)\n",
    "        dens = torch.erf((1 - mu) / sigma) + torch.erf(mu / sigma)\n",
    "        nums = torch.erf((1 - mu) / sigma) + torch.erf((mu - As) / sigma)\n",
    "        return As * nums / dens\n",
    "    \n",
    "    def argmaxone(self, mu, sigma):\n",
    "        from math import erf\n",
    "        import scipy.optimize as so\n",
    "        \n",
    "        const = erf((1 - mu) / sigma)\n",
    "        res = so.minimize_scalar(fun=lambda z: -z * (const + erf((mu - z) / sigma)),\n",
    "                                 bounds=[0, 1],\n",
    "                                 method='bounded')\n",
    "        assert res.success, res\n",
    "        return res.x\n",
    "\n",
    "    def argmax(self, Xs, resolution):\n",
    "        with torch.no_grad():\n",
    "            if self.approxargmax:\n",
    "                from math import ceil\n",
    "                nsamples = int(ceil(1/resolution))\n",
    "                As = torch.rand(size=(Xs.shape[0], nsamples), device=Xs.device)\n",
    "                weirdrv = torch.max(input=self.forward(Xs, As), dim=1, keepdim=True)\n",
    "                fhatstar = weirdrv.values\n",
    "                ahatstar = torch.gather(input=As, dim=1, index=weirdrv.indices)\n",
    "                return fhatstar, ahatstar\n",
    "            else:\n",
    "                mu, sigma = self.distparams(Xs)\n",
    "                ahatstar = torch.Tensor([ [ self.argmaxone(m, s) ] for m, s in zip(mu, sigma) ])\n",
    "                return self.forward(Xs, ahatstar), ahatstar\n",
    "\n",
    "class CorralFastIGW(object):\n",
    "    def __init__(self, *, eta, gammamin, gammamax, nalgos, device):\n",
    "        import numpy\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.eta = eta / nalgos\n",
    "        self.gammas = torch.Tensor(numpy.geomspace(gammamin, gammamax, nalgos)).to(device)\n",
    "        self.invpalgo = torch.Tensor([ self.gammas.shape[0] ] * self.gammas.shape[0]).to(device)\n",
    "        \n",
    "    def update(self, algo, invprop, reward):\n",
    "        import numpy\n",
    "        from scipy import optimize\n",
    "        \n",
    "        assert torch.all(reward >= 0) and torch.all(reward <= 1), reward\n",
    "        \n",
    "        weightedlosses = self.eta * (-reward.squeeze(1)) * invprop.squeeze(1)\n",
    "        newinvpalgo = torch.scatter(input=self.invpalgo,\n",
    "                                    dim=0,\n",
    "                                    index=algo,\n",
    "                                    src=weightedlosses,\n",
    "                                    reduce='add')\n",
    "                                    \n",
    "        # just do this calc on the cpu\n",
    "        invp = newinvpalgo.cpu().numpy() \n",
    "        invp += 1 - numpy.min(invp)\n",
    "        Zlb = 0\n",
    "        Zub = 1\n",
    "        while (numpy.sum(1 / (invp + Zub)) > 1):\n",
    "            Zlb = Zub\n",
    "            Zub *= 2 \n",
    "        root, res = optimize.brentq(lambda z: 1 - numpy.sum(1 / (invp + z)), Zlb, Zub, full_output=True)\n",
    "        assert res.converged, res\n",
    "        \n",
    "        self.invpalgo = torch.Tensor(invp + root, device=self.invpalgo.device)\n",
    "\n",
    "    def sample(self, fhat, X):\n",
    "        N, _ = X.shape\n",
    "\n",
    "        algosampler = torch.distributions.categorical.Categorical(probs=1.0/self.invpalgo, validate_args=False)\n",
    "        algo = algosampler.sample((N,))\n",
    "        invpalgo = torch.gather(input=self.invpalgo.unsqueeze(0).expand(N, -1),\n",
    "                                dim=1,\n",
    "                                index=algo.unsqueeze(1))\n",
    "        gamma = torch.gather(input=self.gammas.unsqueeze(0).expand(N, -1),\n",
    "                             dim=1,\n",
    "                             index=algo.unsqueeze(1))\n",
    "        resolution = 1 / torch.max(gamma).item()\n",
    "        fhatstar, ahatstar = fhat.argmax(X, resolution)\n",
    "        \n",
    "        rando = torch.rand(size=(N, 1), device=X.device)\n",
    "        fhatrando = fhat(X, rando)\n",
    "        probs = 1 / (1 + gamma * (1 - torch.clip(fhatrando / fhatstar, max=1)))\n",
    "        unif = torch.rand(size=(N, 1), device=X.device)\n",
    "        shouldexplore = (unif <= probs).long()\n",
    "        return (ahatstar + shouldexplore * (rando - ahatstar)), algo, invpalgo, shouldexplore, ahatstar\n",
    "\n",
    "def bestconstant(dataset):\n",
    "    import numpy\n",
    "        \n",
    "    constreward = [EasyAcc() for _ in range(100)]\n",
    "    pnosale = [EasyAcc() for _ in range(100)]\n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for _, Ys in generator:\n",
    "        with torch.no_grad():\n",
    "            for n in range(len(constreward)):\n",
    "                z = n / len(constreward)\n",
    "                constreward[n] += torch.mean((Ys >= z).long() * z).item()\n",
    "                pnosale[n] += torch.mean((Ys < z).float()).item()\n",
    "    \n",
    "    return { 'best_constant_reward': max(( (v.mean(), pnosale[n].mean(), n/len(constreward)) for n, v in enumerate(constreward) ))\n",
    "           }\n",
    "\n",
    "def bootstrap(data, stat, conf):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    samples = ceil(3 / conf)\n",
    "    stats = []\n",
    "    for _ in range(samples):\n",
    "        stats.append(stat(np.random.choice(data, len(data))))\n",
    "        \n",
    "    return np.quantile(stats, q=[conf, 1-conf])\n",
    "            \n",
    "def compute_evar_ci(rewards, q):\n",
    "    import numpy as np\n",
    "    \n",
    "    def f(m, data):\n",
    "        return q * np.sum(np.square(np.clip(data - m, a_min=0, a_max=None))) + (1 - q) * np.sum(np.square(np.clip(m - data, a_min=0, a_max=None)))  \n",
    "            \n",
    "    def evar(data):\n",
    "        from scipy.optimize import minimize_scalar\n",
    "        res = minimize_scalar(lambda m: f(m, data), bounds=(np.min(data), np.max(data)), method='bounded')\n",
    "        assert res.success\n",
    "        return res.x\n",
    "    \n",
    "    return bootstrap(rewards, evar, conf=0.05)\n",
    "        \n",
    "def learnOnline(dataset, *, q, seed, batch_size, modelfactory, initlr, tzero, eta, gammamin, gammamax, nalgos):\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = None\n",
    "    l1_loss = torch.nn.L1Loss(reduction='none')\n",
    "    log_loss = torch.nn.BCELoss(reduction='none')\n",
    "    \n",
    "    print('{:<5s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<10s}'.format(\n",
    "            'n', 'loss', 'since last', 'rahat', 'since last', f'evarq({q})', 'since last', 'pnosale', 'since last', 'accept', 'since last', 'dt (sec)'),\n",
    "          flush=True)\n",
    "    avloss, sincelast, acc, accsincelast, avreward, rewardsincelast = [ \n",
    "        EasyPoissonBootstrapAcc(batch_size=batch_size) for _ in range(6) ]\n",
    "    accept, acceptsincelast, pnosale, pnosalesincelast = [\n",
    "        EasyPoissonBootstrapAcc(batch_size=batch_size) for _ in range(4) ]\n",
    "    allrewards, allrewardssincelast = [], []\n",
    "    \n",
    "    for bno, (Xs, ys) in enumerate(generator):\n",
    "        if model is None:\n",
    "            from math import sqrt\n",
    "            model = modelfactory(Xs)\n",
    "            opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: sqrt(tzero) / sqrt(tzero + t))\n",
    "            sampler = CorralFastIGW(eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, device=Xs.device)\n",
    "            start = time.time()\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample, algo, invpalgo, shouldexplore, ahatstar = sampler.sample(model, Xs)\n",
    "            reward = (sample <= ys).long() * sample\n",
    "            allrewards.append(reward.numpy())\n",
    "            allrewardssincelast.append(reward.numpy())\n",
    "            rahat = (ahatstar <= ys).long() * ahatstar\n",
    "        \n",
    "        score = model(Xs, sample)\n",
    "        with torch.no_grad():\n",
    "            factor = q * (score < reward).long() + (1 - q) * (score >= reward).long()\n",
    "        loss = 2 * torch.mean(factor * log_loss(score, reward))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc += torch.mean(rahat).item()\n",
    "            accsincelast += torch.mean(rahat).item()\n",
    "            avreward += torch.mean(reward).item()\n",
    "            rewardsincelast += torch.mean(reward).item()\n",
    "            pnosale += torch.mean((sample > ys).float()).item()\n",
    "            pnosalesincelast += torch.mean((sample > ys).float()).item()\n",
    "            avloss += loss.item()\n",
    "            sincelast += loss.item()\n",
    "            accept += torch.mean(shouldexplore.float()).item()\n",
    "            acceptsincelast += torch.mean(shouldexplore.float()).item()\n",
    "            sampler.update(algo, invpalgo, reward)\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            evar = compute_evar_ci(np.concatenate(allrewards, axis=None), q)\n",
    "            evarsincelast = compute_evar_ci(np.concatenate(allrewardssincelast, axis=None), q)\n",
    "            \n",
    "            print('{:<5d}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<10.5f}'.format(\n",
    "                    avloss.n, avloss.formatci(), sincelast.formatci(), acc.formatci(),\n",
    "                    accsincelast.formatci(), f'[{evar[0]:.4f},{evar[1]:.4f}]', f'[{evarsincelast[0]:.4f},{evarsincelast[1]:.4f}]',\n",
    "                    pnosale.formatci(), pnosalesincelast.formatci(),\n",
    "                    accept.formatci(), acceptsincelast.formatci(),\n",
    "                    time.time() - start),\n",
    "                  flush=True)\n",
    "            sincelast, accsincelast, rewardsincelast, acceptsincelast, pundersincelast = [ \n",
    "                EasyPoissonBootstrapAcc(batch_size=batch_size) for _ in range(5) ]\n",
    "            allrewardssincelast = []\n",
    "            print(f'sampler.palgo = { 1/sampler.invpalgo }')\n",
    "            \n",
    "    evar = compute_evar_ci(np.concatenate(allrewards, axis=None), q)\n",
    "    evarsincelast = compute_evar_ci(np.concatenate(allrewardssincelast, axis=None), q)\n",
    "    print('{:<5d}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<10.5f}'.format(\n",
    "            avloss.n, avloss.formatci(), sincelast.formatci(), acc.formatci(),\n",
    "            accsincelast.formatci(), f'[{evar[0]:.4f},{evar[1]:.4f}]', f'[{evarsincelast[0]:.4f},{evarsincelast[1]:.4f}]',\n",
    "            pnosale.formatci(), pnosalesincelast.formatci(),\n",
    "            accept.formatci(), acceptsincelast.formatci(),\n",
    "            time.time() - start),\n",
    "          flush=True)\n",
    "    print(f'sampler.palgo = { 1/sampler.invpalgo }')\n",
    "    print(f'0.2: {compute_evar_ci(np.concatenate(allrewards, axis=None), 0.2)} 0.5: {compute_evar_ci(np.concatenate(allrewards, axis=None), 0.5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2273ac",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Diamonds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e20988a7",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_reward': (0.07153903490290364, 0.6593379300189329, 0.21)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = ArffToPytorch('diamondprices.arff', target='price', skipcol=['id', 'address'], skiprow=lambda z: z['price'] > 1e6)\n",
    "bestconstant(mydata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dabf7d3",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def doit():\n",
    "    # see tune-housing-mean.diamond.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.040858591928985916, 7.375547794458968, 0.14670396451105366, 1, 2048, 5\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu')) \n",
    "doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e089b21",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def doit():\n",
    "    # see tune-housing-expectile.diamond.0_2.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.04832550041557601, 9.50230257811169, 0.6867451075796088, 1, 4096, 4\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu')) \n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51311ce6",
   "metadata": {},
   "source": [
    "## Perth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bb9ce211",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_reward': (0.2700000074873497, 0.25, 0.36)}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = ArffToPytorch('perthhouseprices.arff', target='price', skipcol=['id', 'address'], skiprow=lambda z: z['price'] > 1e6)\n",
    "bestconstant(mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9af0613",
   "metadata": {},
   "source": [
    "### risk-neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "73c06cf4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.5)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.2105,0.8419]  \t[0.2105,0.8419]  \t[0.0868,0.3471]  \t[0.0868,0.3471]  \t[0.0726,0.3627]  \t[0.1157,0.3763]  \t[0.1875,0.7500]  \t[0.1875,0.7500]  \t[0.0938,0.3750]  \t[0.0938,0.3750]  \t0.25251   \n",
      "sampler.palgo = tensor([0.2033, 0.2012, 0.2011, 0.2012, 0.1932])\n",
      "2    \t[0.2863,0.7424]  \t[0.1752,0.7007]  \t[0.1339,0.3410]  \t[0.0913,0.3653]  \t[0.1187,0.2882]  \t[0.0608,0.3045]  \t[0.3199,0.8051]  \t[0.3199,0.8051]  \t[0.1406,0.3594]  \t[0.0938,0.3750]  \t0.42285   \n",
      "sampler.palgo = tensor([0.1980, 0.2045, 0.1959, 0.1960, 0.2056])\n",
      "3    \t[0.3283,0.6875]  \t[0.1775,0.7099]  \t[0.1414,0.2897]  \t[0.0673,0.2693]  \t[0.0928,0.2812]  \t[0.0583,0.2829]  \t[0.3745,0.7971]  \t[0.3745,0.7971]  \t[0.1661,0.3339]  \t[0.0938,0.3750]  \t0.58693   \n",
      "sampler.palgo = tensor([0.2002, 0.1995, 0.1994, 0.1914, 0.2095])\n",
      "5    \t[0.3910,0.6535]  \t[0.3129,0.8002]  \t[0.1681,0.2868]  \t[0.1394,0.3637]  \t[0.1428,0.2778]  \t[0.1610,0.3233]  \t[0.3994,0.6820]  \t[0.3994,0.6820]  \t[0.1556,0.2631]  \t[0.0703,0.1797]  \t0.78450   \n",
      "sampler.palgo = tensor([0.2042, 0.1942, 0.2033, 0.1792, 0.2191])\n",
      "9    \t[0.4158,0.6157]  \t[0.3877,0.6594]  \t[0.1809,0.2748]  \t[0.1580,0.2854]  \t[0.1788,0.2591]  \t[0.1828,0.2815]  \t[0.3835,0.5751]  \t[0.3835,0.5751]  \t[0.1596,0.2520]  \t[0.1484,0.2969]  \t0.96316   \n",
      "sampler.palgo = tensor([0.2018, 0.2006, 0.2018, 0.1803, 0.2155])\n",
      "17   \t[0.4467,0.5916]  \t[0.3980,0.6058]  \t[0.1876,0.2521]  \t[0.1615,0.2445]  \t[0.1896,0.2409]  \t[0.1555,0.2483]  \t[0.4372,0.6030]  \t[0.4372,0.6030]  \t[0.1635,0.2244]  \t[0.1304,0.2148]  \t1.16676   \n",
      "sampler.palgo = tensor([0.2047, 0.1964, 0.1824, 0.1851, 0.2314])\n",
      "33   \t[0.4656,0.5860]  \t[0.4750,0.6273]  \t[0.2060,0.2581]  \t[0.2135,0.2853]  \t[0.2073,0.2549]  \t[0.2122,0.2797]  \t[0.4282,0.5578]  \t[0.4282,0.5578]  \t[0.1837,0.2319]  \t[0.1862,0.2568]  \t1.44306   \n",
      "sampler.palgo = tensor([0.2508, 0.1969, 0.1618, 0.1764, 0.2141])\n",
      "65   \t[0.5070,0.5801]  \t[0.4906,0.6220]  \t[0.2281,0.2629]  \t[0.2258,0.2873]  \t[0.2255,0.2604]  \t[0.2396,0.2859]  \t[0.4155,0.4854]  \t[0.4155,0.4854]  \t[0.1834,0.2193]  \t[0.1831,0.2393]  \t1.78238   \n",
      "sampler.palgo = tensor([0.3376, 0.1481, 0.1437, 0.2070, 0.1636])\n",
      "129  \t[0.5332,0.5900]  \t[0.5333,0.6170]  \t[0.2557,0.2872]  \t[0.2729,0.3178]  \t[0.2535,0.2745]  \t[0.2771,0.3021]  \t[0.3410,0.3800]  \t[0.3410,0.3800]  \t[0.1770,0.2019]  \t[0.1618,0.1942]  \t2.31013   \n",
      "sampler.palgo = tensor([0.2334, 0.1672, 0.1753, 0.2631, 0.1610])\n",
      "257  \t[0.5562,0.5940]  \t[0.5428,0.6060]  \t[0.2682,0.2894]  \t[0.2660,0.2945]  \t[0.2620,0.2771]  \t[0.2630,0.2862]  \t[0.3272,0.3560]  \t[0.3272,0.3560]  \t[0.1544,0.1683]  \t[0.1234,0.1415]  \t3.18035   \n",
      "sampler.palgo = tensor([0.2494, 0.0820, 0.2626, 0.2812, 0.1247])\n",
      "513  \t[0.5632,0.5898]  \t[0.5630,0.6023]  \t[0.2825,0.2969]  \t[0.2922,0.3133]  \t[0.2767,0.2870]  \t[0.2880,0.3031]  \t[0.3091,0.3288]  \t[0.3091,0.3288]  \t[0.1273,0.1362]  \t[0.0966,0.1072]  \t4.67857   \n",
      "sampler.palgo = tensor([0.0838, 0.0561, 0.1659, 0.5186, 0.1756])\n",
      "1025 \t[0.5747,0.5930]  \t[0.5799,0.6068]  \t[0.2973,0.3070]  \t[0.3088,0.3237]  \t[0.2947,0.3015]  \t[0.3077,0.3180]  \t[0.2814,0.2918]  \t[0.2814,0.2918]  \t[0.0967,0.1022]  \t[0.0651,0.0706]  \t7.66447   \n",
      "sampler.palgo = tensor([0.0319, 0.0397, 0.1366, 0.2240, 0.5679])\n",
      "2049 \t[0.5822,0.5944]  \t[0.5854,0.6014]  \t[0.3057,0.3127]  \t[0.3125,0.3220]  \t[0.3045,0.3096]  \t[0.3122,0.3194]  \t[0.2593,0.2655]  \t[0.2593,0.2655]  \t[0.0662,0.0693]  \t[0.0351,0.0375]  \t13.33620  \n",
      "sampler.palgo = tensor([0.0128, 0.0274, 0.0420, 0.5601, 0.3577])\n",
      "3693 \t[0.5888,0.5981]  \t[0.5915,0.6056]  \t[0.3142,0.3192]  \t[0.3215,0.3297]  \t[0.3131,0.3167]  \t[0.3219,0.3286]  \t[0.2349,0.2391]  \t[0.2349,0.2391]  \t[0.0526,0.0546]  \t[0.0351,0.0370]  \t22.57104  \n",
      "sampler.palgo = tensor([0.0059, 0.0210, 0.0426, 0.8024, 0.1281])\n",
      "0.2: [0.18095918 0.18508843] 0.5: [0.31375538 0.31677528]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-housing-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.06552911464593603, 9.295089626986172, 0.4415259706643872, 8, 2048, 5\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=True)) \n",
    "doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af593975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.5)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.1966,0.7862]  \t[0.1966,0.7862]  \t[0.0868,0.3474]  \t[0.0868,0.3474]  \t[0.0788,0.3299]  \t[0.0979,0.2945]  \t[0.1875,0.7500]  \t[0.1875,0.7500]  \t[0.0469,0.1875]  \t[0.0469,0.1875]  \t0.18036   \n",
      "sampler.palgo = tensor([0.1996, 0.2021, 0.2021, 0.2021, 0.1941])\n",
      "2    \t[0.3159,0.8004]  \t[0.2220,0.8882]  \t[0.1461,0.3681]  \t[0.1060,0.4239]  \t[0.1531,0.3347]  \t[0.1135,0.3890]  \t[0.2422,0.6332]  \t[0.2422,0.6332]  \t[0.1090,0.2656]  \t[0.0938,0.3750]  \t0.35101   \n",
      "sampler.palgo = tensor([0.1916, 0.2101, 0.2178, 0.1940, 0.1866])\n",
      "3    \t[0.3424,0.7104]  \t[0.1784,0.7137]  \t[0.1447,0.3087]  \t[0.0660,0.2639]  \t[0.1677,0.2919]  \t[0.1142,0.2933]  \t[0.3380,0.6638]  \t[0.3380,0.6638]  \t[0.0727,0.1771]  \t[0.0000,0.0000]  \t0.52186   \n",
      "sampler.palgo = tensor([0.1869, 0.2131, 0.2207, 0.1972, 0.1821])\n",
      "5    \t[0.4002,0.6734]  \t[0.3119,0.8038]  \t[0.1731,0.2944]  \t[0.1368,0.3571]  \t[0.1725,0.2858]  \t[0.1646,0.3505]  \t[0.3433,0.6073]  \t[0.3433,0.6073]  \t[0.1030,0.1909]  \t[0.1016,0.2734]  \t0.71745   \n",
      "sampler.palgo = tensor([0.1807, 0.2046, 0.2477, 0.1969, 0.1702])\n",
      "9    \t[0.4318,0.6209]  \t[0.3965,0.6687]  \t[0.1863,0.2744]  \t[0.1752,0.2938]  \t[0.1954,0.2589]  \t[0.1668,0.2746]  \t[0.3469,0.5262]  \t[0.3469,0.5262]  \t[0.0920,0.1720]  \t[0.0617,0.2031]  \t0.92138   \n",
      "sampler.palgo = tensor([0.1726, 0.2175, 0.2344, 0.2066, 0.1689])\n",
      "17   \t[0.4595,0.6135]  \t[0.4139,0.6350]  \t[0.2031,0.2674]  \t[0.1814,0.2783]  \t[0.2129,0.2595]  \t[0.1989,0.2761]  \t[0.3700,0.5187]  \t[0.3700,0.5187]  \t[0.1093,0.1657]  \t[0.1090,0.1760]  \t1.16916   \n",
      "sampler.palgo = tensor([0.1589, 0.2057, 0.2613, 0.2104, 0.1637])\n",
      "33   \t[0.4842,0.6131]  \t[0.4969,0.6725]  \t[0.2273,0.2871]  \t[0.2468,0.3346]  \t[0.2374,0.2752]  \t[0.2572,0.2969]  \t[0.2992,0.3784]  \t[0.2992,0.3784]  \t[0.1116,0.1463]  \t[0.0986,0.1436]  \t1.49636   \n",
      "sampler.palgo = tensor([0.1496, 0.2162, 0.2200, 0.2170, 0.1972])\n",
      "65   \t[0.5248,0.6042]  \t[0.5165,0.6510]  \t[0.2543,0.2935]  \t[0.2580,0.3246]  \t[0.2597,0.2869]  \t[0.2762,0.3112]  \t[0.2694,0.3174]  \t[0.2694,0.3174]  \t[0.1007,0.1241]  \t[0.0823,0.1167]  \t2.06834   \n",
      "sampler.palgo = tensor([0.1550, 0.1675, 0.2456, 0.1959, 0.2359])\n",
      "129  \t[0.5491,0.6095]  \t[0.5451,0.6262]  \t[0.2735,0.3041]  \t[0.2788,0.3239]  \t[0.2756,0.2943]  \t[0.2835,0.3093]  \t[0.2450,0.2735]  \t[0.2450,0.2735]  \t[0.0879,0.1013]  \t[0.0695,0.0872]  \t2.97367   \n",
      "sampler.palgo = tensor([0.1228, 0.1687, 0.2098, 0.2310, 0.2678])\n",
      "257  \t[0.5611,0.6003]  \t[0.5394,0.5972]  \t[0.2774,0.2981]  \t[0.2654,0.2940]  \t[0.2767,0.2880]  \t[0.2700,0.2926]  \t[0.2724,0.2960]  \t[0.2724,0.2960]  \t[0.0970,0.1087]  \t[0.1007,0.1146]  \t4.62829   \n",
      "sampler.palgo = tensor([0.0708, 0.2254, 0.2480, 0.0831, 0.3728])\n",
      "513  \t[0.5648,0.5915]  \t[0.5618,0.6020]  \t[0.2880,0.3013]  \t[0.2931,0.3142]  \t[0.2837,0.2948]  \t[0.2880,0.3031]  \t[0.2753,0.2924]  \t[0.2753,0.2924]  \t[0.1011,0.1100]  \t[0.1055,0.1165]  \t7.65436   \n",
      "sampler.palgo = tensor([0.0599, 0.3763, 0.2061, 0.0799, 0.2777])\n",
      "1025 \t[0.5767,0.5940]  \t[0.5800,0.6077]  \t[0.3008,0.3105]  \t[0.3102,0.3257]  \t[0.2980,0.3044]  \t[0.3099,0.3192]  \t[0.2452,0.2555]  \t[0.2452,0.2555]  \t[0.0900,0.0952]  \t[0.0771,0.0832]  \t14.29536  \n",
      "sampler.palgo = tensor([0.0449, 0.2208, 0.3785, 0.1543, 0.2015])\n",
      "2049 \t[0.5814,0.5932]  \t[0.5820,0.5980]  \t[0.3082,0.3148]  \t[0.3136,0.3226]  \t[0.3058,0.3100]  \t[0.3105,0.3176]  \t[0.2350,0.2411]  \t[0.2350,0.2411]  \t[0.0852,0.0892]  \t[0.0791,0.0833]  \t26.90436  \n",
      "sampler.palgo = tensor([0.0189, 0.1327, 0.1211, 0.6192, 0.1082])\n",
      "3693 \t[0.5850,0.5937]  \t[0.5840,0.5977]  \t[0.3130,0.3179]  \t[0.3164,0.3243]  \t[0.3103,0.3137]  \t[0.3146,0.3204]  \t[0.2334,0.2380]  \t[0.2334,0.2380]  \t[0.0737,0.0758]  \t[0.0581,0.0603]  \t45.42424  \n",
      "sampler.palgo = tensor([0.0098, 0.0320, 0.1000, 0.6056, 0.2526])\n",
      "0.2: [0.1801786  0.18415793] 0.5: [0.31021835 0.31382755]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-housing-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.06552911464593603, 9.295089626986172, 0.4415259706643872, 8, 2048, 5\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=False)) \n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c5ba8d0",
   "metadata": {},
   "source": [
    "### risk-averse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17a326e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.2)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.1464,0.5854]  \t[0.1464,0.5854]  \t[0.0867,0.3470]  \t[0.0867,0.3470]  \t[0.0346,0.2943]  \t[0.0346,0.1982]  \t[0.1875,0.7500]  \t[0.1875,0.7500]  \t[0.0469,0.1875]  \t[0.0469,0.1875]  \t0.15335   \n",
      "sampler.palgo = tensor([0.2012, 0.2012, 0.2012, 0.2012, 0.1953])\n",
      "2    \t[0.2088,0.5375]  \t[0.1335,0.5338]  \t[0.1484,0.3733]  \t[0.1089,0.4355]  \t[0.0433,0.1655]  \t[0.0518,0.3267]  \t[0.2422,0.6332]  \t[0.2422,0.6332]  \t[0.0703,0.1797]  \t[0.0469,0.1875]  \t0.30734   \n",
      "sampler.palgo = tensor([0.2016, 0.2055, 0.2015, 0.1956, 0.1958])\n",
      "3    \t[0.2459,0.5016]  \t[0.1380,0.5521]  \t[0.1435,0.3085]  \t[0.0627,0.2507]  \t[0.0489,0.1356]  \t[0.0154,0.1308]  \t[0.3380,0.6638]  \t[0.3380,0.6638]  \t[0.0831,0.1669]  \t[0.0469,0.1875]  \t0.45431   \n",
      "sampler.palgo = tensor([0.1981, 0.2019, 0.2039, 0.1980, 0.1982])\n",
      "5    \t[0.2572,0.4357]  \t[0.1704,0.4397]  \t[0.1863,0.3213]  \t[0.1672,0.4265]  \t[0.0795,0.1773]  \t[0.1089,0.2614]  \t[0.2803,0.4944]  \t[0.2803,0.4944]  \t[0.0936,0.1564]  \t[0.0703,0.1797]  \t0.61313   \n",
      "sampler.palgo = tensor([0.1997, 0.2033, 0.2010, 0.1950, 0.2009])\n",
      "9    \t[0.2432,0.3547]  \t[0.1890,0.3181]  \t[0.1952,0.2846]  \t[0.1756,0.2936]  \t[0.1021,0.1717]  \t[0.1011,0.1949]  \t[0.2252,0.3615]  \t[0.2252,0.3615]  \t[0.1146,0.1875]  \t[0.1172,0.2578]  \t0.81738   \n",
      "sampler.palgo = tensor([0.2111, 0.2061, 0.1962, 0.1935, 0.1931])\n",
      "17   \t[0.2376,0.3238]  \t[0.1939,0.2939]  \t[0.2028,0.2706]  \t[0.1753,0.2676]  \t[0.1119,0.1706]  \t[0.1301,0.2003]  \t[0.1727,0.2574]  \t[0.1727,0.2574]  \t[0.0781,0.1167]  \t[0.0176,0.0410]  \t1.06353   \n",
      "sampler.palgo = tensor([0.2131, 0.1847, 0.2158, 0.1922, 0.1942])\n",
      "33   \t[0.2434,0.3084]  \t[0.2416,0.3266]  \t[0.2235,0.2819]  \t[0.2369,0.3181]  \t[0.1310,0.1639]  \t[0.1237,0.1883]  \t[0.1870,0.2396]  \t[0.1870,0.2396]  \t[0.0848,0.1147]  \t[0.0771,0.1291]  \t1.32816   \n",
      "sampler.palgo = tensor([0.1706, 0.1772, 0.2009, 0.2666, 0.1847])\n",
      "65   \t[0.2653,0.3027]  \t[0.2558,0.3253]  \t[0.2458,0.2831]  \t[0.2456,0.3083]  \t[0.1392,0.1724]  \t[0.1402,0.1883]  \t[0.2028,0.2361]  \t[0.2028,0.2361]  \t[0.0988,0.1219]  \t[0.1045,0.1398]  \t1.70213   \n",
      "sampler.palgo = tensor([0.1469, 0.1706, 0.1903, 0.3050, 0.1872])\n",
      "129  \t[0.2702,0.3008]  \t[0.2667,0.3046]  \t[0.2652,0.2943]  \t[0.2709,0.3127]  \t[0.1603,0.1795]  \t[0.1686,0.2004]  \t[0.1895,0.2096]  \t[0.1895,0.2096]  \t[0.1053,0.1187]  \t[0.1054,0.1250]  \t2.47441   \n",
      "sampler.palgo = tensor([0.1704, 0.2371, 0.1560, 0.2493, 0.1872])\n",
      "257  \t[0.2786,0.2980]  \t[0.2697,0.3010]  \t[0.2818,0.3019]  \t[0.2818,0.3097]  \t[0.1717,0.1867]  \t[0.1784,0.1998]  \t[0.1790,0.1944]  \t[0.1790,0.1944]  \t[0.1083,0.1156]  \t[0.1011,0.1189]  \t3.44794   \n",
      "sampler.palgo = tensor([0.0806, 0.2597, 0.1196, 0.3581, 0.1820])\n",
      "513  \t[0.2752,0.2884]  \t[0.2690,0.2877]  \t[0.2891,0.3030]  \t[0.2923,0.3124]  \t[0.1900,0.2018]  \t[0.2051,0.2217]  \t[0.1492,0.1588]  \t[0.1492,0.1588]  \t[0.0914,0.0980]  \t[0.0735,0.0813]  \t5.28285   \n",
      "sampler.palgo = tensor([0.0454, 0.1403, 0.1167, 0.5568, 0.1408])\n",
      "1025 \t[0.2662,0.2739]  \t[0.2527,0.2648]  \t[0.2901,0.2992]  \t[0.2879,0.3018]  \t[0.2069,0.2130]  \t[0.2185,0.2284]  \t[0.1133,0.1189]  \t[0.1133,0.1189]  \t[0.0689,0.0719]  \t[0.0441,0.0488]  \t8.97566   \n",
      "sampler.palgo = tensor([0.0450, 0.0896, 0.1057, 0.5079, 0.2517])\n",
      "2049 \t[0.2649,0.2704]  \t[0.2619,0.2687]  \t[0.2958,0.3023]  \t[0.2995,0.3074]  \t[0.2167,0.2212]  \t[0.2250,0.2310]  \t[0.0999,0.1036]  \t[0.0999,0.1036]  \t[0.0547,0.0569]  \t[0.0396,0.0427]  \t15.15835  \n",
      "sampler.palgo = tensor([0.0176, 0.0350, 0.1068, 0.6976, 0.1430])\n",
      "3693 \t[0.2605,0.2647]  \t[0.2525,0.2584]  \t[0.2965,0.3011]  \t[0.2946,0.3017]  \t[0.2218,0.2255]  \t[0.2266,0.2311]  \t[0.0854,0.0875]  \t[0.0854,0.0875]  \t[0.0455,0.0469]  \t[0.0334,0.0350]  \t24.83890  \n",
      "sampler.palgo = tensor([0.0117, 0.0343, 0.0598, 0.8352, 0.0590])\n",
      "0.2: [0.22212842 0.22486196] 0.5: [0.2960474  0.29877916]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "     # see tune-housing-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.05818878393226181, 3.006187783398874, 0.32654466058766385, 8, 4096, 5\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=True))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "451f2ae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.2)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.1437,0.5747]  \t[0.1437,0.5747]  \t[0.0868,0.3474]  \t[0.0868,0.3474]  \t[0.0301,0.1781]  \t[0.0301,0.2020]  \t[0.1875,0.7500]  \t[0.1875,0.7500]  \t[0.0469,0.1875]  \t[0.0469,0.1875]  \t0.18701   \n",
      "sampler.palgo = tensor([0.1997, 0.2016, 0.2016, 0.2016, 0.1956])\n",
      "2    \t[0.2086,0.5357]  \t[0.1354,0.5416]  \t[0.1484,0.3734]  \t[0.1088,0.4353]  \t[0.0572,0.1887]  \t[0.0572,0.2889]  \t[0.2422,0.6332]  \t[0.2422,0.6332]  \t[0.1090,0.2656]  \t[0.0938,0.3750]  \t0.37624   \n",
      "sampler.palgo = tensor([0.1937, 0.2077, 0.2133, 0.1955, 0.1899])\n",
      "3    \t[0.2468,0.4999]  \t[0.1393,0.5573]  \t[0.1436,0.3086]  \t[0.0628,0.2511]  \t[0.0491,0.1292]  \t[0.0000,0.1312]  \t[0.3380,0.6638]  \t[0.3380,0.6638]  \t[0.0727,0.1771]  \t[0.0000,0.0000]  \t0.56476   \n",
      "sampler.palgo = tensor([0.1904, 0.2099, 0.2154, 0.1977, 0.1866])\n",
      "5    \t[0.2565,0.4371]  \t[0.1703,0.4368]  \t[0.1864,0.3213]  \t[0.1671,0.4263]  \t[0.0784,0.1832]  \t[0.0930,0.2914]  \t[0.2803,0.4944]  \t[0.2803,0.4944]  \t[0.0688,0.1286]  \t[0.0234,0.0938]  \t0.81323   \n",
      "sampler.palgo = tensor([0.1875, 0.2061, 0.2270, 0.1995, 0.1800])\n",
      "9    \t[0.2465,0.3599]  \t[0.1991,0.3368]  \t[0.2031,0.2922]  \t[0.1856,0.3095]  \t[0.1094,0.1684]  \t[0.1113,0.2236]  \t[0.2115,0.3528]  \t[0.2115,0.3528]  \t[0.0676,0.1199]  \t[0.0463,0.1523]  \t1.14081   \n",
      "sampler.palgo = tensor([0.1826, 0.2066, 0.2260, 0.2034, 0.1813])\n",
      "17   \t[0.2542,0.3453]  \t[0.2184,0.3343]  \t[0.2139,0.2870]  \t[0.1885,0.2940]  \t[0.1183,0.1680]  \t[0.1147,0.1819]  \t[0.2175,0.3055]  \t[0.2175,0.3055]  \t[0.0790,0.1223]  \t[0.0820,0.1388]  \t1.46151   \n",
      "sampler.palgo = tensor([0.1697, 0.2096, 0.2316, 0.2167, 0.1725])\n",
      "33   \t[0.2534,0.3191]  \t[0.2436,0.3278]  \t[0.2253,0.2839]  \t[0.2291,0.3108]  \t[0.1365,0.1681]  \t[0.1376,0.2065]  \t[0.2049,0.2579]  \t[0.2049,0.2579]  \t[0.1046,0.1320]  \t[0.1210,0.1670]  \t1.89930   \n",
      "sampler.palgo = tensor([0.1712, 0.2053, 0.2191, 0.2151, 0.1894])\n",
      "65   \t[0.2701,0.3093]  \t[0.2570,0.3270]  \t[0.2493,0.2896]  \t[0.2543,0.3194]  \t[0.1455,0.1724]  \t[0.1347,0.1933]  \t[0.2124,0.2496]  \t[0.2124,0.2496]  \t[0.1233,0.1474]  \t[0.1278,0.1696]  \t2.49608   \n",
      "sampler.palgo = tensor([0.1457, 0.1880, 0.2331, 0.2006, 0.2327])\n",
      "129  \t[0.2765,0.3078]  \t[0.2732,0.3121]  \t[0.2699,0.2994]  \t[0.2749,0.3163]  \t[0.1605,0.1798]  \t[0.1757,0.2101]  \t[0.1958,0.2195]  \t[0.1958,0.2195]  \t[0.1047,0.1206]  \t[0.0826,0.1004]  \t3.41637   \n",
      "sampler.palgo = tensor([0.1076, 0.1950, 0.2392, 0.1973, 0.2608])\n",
      "257  \t[0.2861,0.3063]  \t[0.2779,0.3092]  \t[0.2827,0.3023]  \t[0.2774,0.3069]  \t[0.1672,0.1859]  \t[0.1699,0.1944]  \t[0.1937,0.2109]  \t[0.1937,0.2109]  \t[0.1068,0.1175]  \t[0.1020,0.1177]  \t5.25527   \n",
      "sampler.palgo = tensor([0.0665, 0.2576, 0.2561, 0.1042, 0.3155])\n",
      "513  \t[0.2824,0.2963]  \t[0.2754,0.2944]  \t[0.2903,0.3042]  \t[0.2944,0.3146]  \t[0.1843,0.1962]  \t[0.1989,0.2119]  \t[0.1684,0.1798]  \t[0.1684,0.1798]  \t[0.1026,0.1108]  \t[0.0967,0.1074]  \t8.53518   \n",
      "sampler.palgo = tensor([0.0539, 0.3407, 0.2342, 0.0884, 0.2828])\n",
      "1025 \t[0.2729,0.2809]  \t[0.2583,0.2711]  \t[0.2931,0.3021]  \t[0.2925,0.3062]  \t[0.2004,0.2081]  \t[0.2148,0.2234]  \t[0.1315,0.1383]  \t[0.1315,0.1383]  \t[0.0893,0.0942]  \t[0.0743,0.0802]  \t14.05425  \n",
      "sampler.palgo = tensor([0.0445, 0.2357, 0.3986, 0.0882, 0.2330])\n",
      "2049 \t[0.2634,0.2690]  \t[0.2525,0.2590]  \t[0.2935,0.2995]  \t[0.2917,0.2995]  \t[0.2118,0.2174]  \t[0.2196,0.2254]  \t[0.1033,0.1069]  \t[0.1033,0.1069]  \t[0.0735,0.0763]  \t[0.0557,0.0590]  \t26.61425  \n",
      "sampler.palgo = tensor([0.0218, 0.1683, 0.1052, 0.0316, 0.6730])\n",
      "3693 \t[0.2627,0.2669]  \t[0.2593,0.2655]  \t[0.2977,0.3022]  \t[0.3001,0.3073]  \t[0.2179,0.2214]  \t[0.2237,0.2290]  \t[0.0959,0.0984]  \t[0.0959,0.0984]  \t[0.0675,0.0693]  \t[0.0591,0.0615]  \t44.61259  \n",
      "sampler.palgo = tensor([0.0121, 0.1593, 0.0558, 0.0415, 0.7314])\n",
      "0.2: [0.21753953 0.22071865] 0.5: [0.29579787 0.29797169]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "     # see tune-housing-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.05818878393226181, 3.006187783398874, 0.32654466058766385, 8, 4096, 5\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=False))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1e92d2",
   "metadata": {},
   "source": [
    "## King County"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21066b68",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_reward': (0.2006168637956892, 0.40995039684431894, 0.34)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = ArffToPytorch('kingcountyprices.arff', target='price', skipcol=['id'], skiprow=lambda z: z['price'] > 1e6)\n",
    "bestconstant(mydata)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b04362b",
   "metadata": {},
   "source": [
    "### risk-neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0ccd0cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.5)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.1194,0.4775]  \t[0.1194,0.4775]  \t[0.0216,0.0863]  \t[0.0216,0.0863]  \t[0.0000,0.1151]  \t[0.0000,0.1727]  \t[0.3281,1.3125]  \t[0.3281,1.3125]  \t[0.0938,0.3750]  \t[0.0938,0.3750]  \t0.11853   \n",
      "sampler.palgo = tensor([0.1996, 0.1996, 0.2016, 0.1996, 0.1996])\n",
      "2    \t[0.2381,0.6035]  \t[0.1986,0.7944]  \t[0.0783,0.2101]  \t[0.0853,0.3413]  \t[0.0930,0.1908]  \t[0.1836,0.3055]  \t[0.2965,0.8133]  \t[0.2965,0.8133]  \t[0.1016,0.2734]  \t[0.0469,0.1875]  \t0.26974   \n",
      "sampler.palgo = tensor([0.1993, 0.2006, 0.2015, 0.1993, 0.1993])\n",
      "3    \t[0.2566,0.5426]  \t[0.1441,0.5766]  \t[0.0961,0.1986]  \t[0.0555,0.2220]  \t[0.0866,0.2145]  \t[0.0476,0.2643]  \t[0.3852,0.8049]  \t[0.3852,0.8049]  \t[0.2021,0.4068]  \t[0.1875,0.7500]  \t0.42706   \n",
      "sampler.palgo = tensor([0.1997, 0.1997, 0.2005, 0.2001, 0.2000])\n",
      "5    \t[0.3038,0.5259]  \t[0.2352,0.5965]  \t[0.1057,0.1914]  \t[0.0859,0.2119]  \t[0.1075,0.1907]  \t[0.0724,0.2086]  \t[0.3991,0.7008]  \t[0.3991,0.7008]  \t[0.1405,0.2689]  \t[0.0234,0.0938]  \t0.59556   \n",
      "sampler.palgo = tensor([0.1990, 0.1989, 0.2008, 0.2033, 0.1980])\n",
      "9    \t[0.3480,0.5071]  \t[0.3553,0.6063]  \t[0.1331,0.1985]  \t[0.1507,0.2558]  \t[0.1327,0.1877]  \t[0.1408,0.2244]  \t[0.3779,0.5629]  \t[0.3779,0.5629]  \t[0.1111,0.1915]  \t[0.0625,0.1178]  \t0.75213   \n",
      "sampler.palgo = tensor([0.1993, 0.2018, 0.1965, 0.2004, 0.2019])\n",
      "17   \t[0.3993,0.5504]  \t[0.3916,0.6052]  \t[0.1663,0.2272]  \t[0.1716,0.2652]  \t[0.1651,0.2150]  \t[0.1786,0.2683]  \t[0.3986,0.5515]  \t[0.3986,0.5515]  \t[0.1158,0.1659]  \t[0.0876,0.1582]  \t0.94782   \n",
      "sampler.palgo = tensor([0.2017, 0.1973, 0.2047, 0.1993, 0.1970])\n",
      "33   \t[0.4212,0.5340]  \t[0.4216,0.5789]  \t[0.1821,0.2311]  \t[0.1879,0.2668]  \t[0.1805,0.2301]  \t[0.1920,0.2468]  \t[0.3674,0.4691]  \t[0.3674,0.4691]  \t[0.1325,0.1771]  \t[0.1416,0.2033]  \t1.21567   \n",
      "sampler.palgo = tensor([0.2015, 0.1996, 0.2148, 0.1915, 0.1926])\n",
      "65   \t[0.4639,0.5327]  \t[0.4558,0.5740]  \t[0.2137,0.2470]  \t[0.2272,0.2838]  \t[0.2123,0.2369]  \t[0.2354,0.2638]  \t[0.2641,0.3206]  \t[0.2641,0.3206]  \t[0.1038,0.1265]  \t[0.0625,0.0865]  \t1.53020   \n",
      "sampler.palgo = tensor([0.1805, 0.2013, 0.2129, 0.1984, 0.2069])\n",
      "129  \t[0.4855,0.5433]  \t[0.4933,0.5672]  \t[0.2366,0.2655]  \t[0.2528,0.2900]  \t[0.2374,0.2561]  \t[0.2564,0.2810]  \t[0.2140,0.2509]  \t[0.2140,0.2509]  \t[0.0932,0.1099]  \t[0.0791,0.0966]  \t1.99847   \n",
      "sampler.palgo = tensor([0.1601, 0.2103, 0.2242, 0.1973, 0.2082])\n",
      "257  \t[0.5117,0.5475]  \t[0.5050,0.5587]  \t[0.2545,0.2733]  \t[0.2554,0.2808]  \t[0.2503,0.2611]  \t[0.2565,0.2751]  \t[0.1940,0.2132]  \t[0.1940,0.2132]  \t[0.0886,0.0992]  \t[0.0777,0.0914]  \t2.70234   \n",
      "sampler.palgo = tensor([0.1628, 0.1925, 0.2297, 0.2039, 0.2111])\n",
      "513  \t[0.5186,0.5444]  \t[0.5206,0.5582]  \t[0.2637,0.2770]  \t[0.2699,0.2901]  \t[0.2607,0.2696]  \t[0.2672,0.2789]  \t[0.1717,0.1847]  \t[0.1717,0.1847]  \t[0.0768,0.0838]  \t[0.0630,0.0703]  \t4.02857   \n",
      "sampler.palgo = tensor([0.1538, 0.1509, 0.2774, 0.2160, 0.2019])\n",
      "1025 \t[0.5291,0.5451]  \t[0.5310,0.5558]  \t[0.2745,0.2830]  \t[0.2806,0.2945]  \t[0.2709,0.2771]  \t[0.2778,0.2889]  \t[0.1747,0.1818]  \t[0.1747,0.1818]  \t[0.0746,0.0788]  \t[0.0697,0.0750]  \t6.34677   \n",
      "sampler.palgo = tensor([0.0934, 0.1582, 0.3941, 0.1914, 0.1630])\n",
      "2049 \t[0.5344,0.5460]  \t[0.5369,0.5513]  \t[0.2805,0.2871]  \t[0.2854,0.2937]  \t[0.2762,0.2819]  \t[0.2820,0.2884]  \t[0.1814,0.1863]  \t[0.1814,0.1863]  \t[0.0676,0.0703]  \t[0.0599,0.0635]  \t10.56402  \n",
      "sampler.palgo = tensor([0.0580, 0.1083, 0.4345, 0.1679, 0.2312])\n",
      "2519 \t[0.5368,0.5476]  \t[0.5397,0.5642]  \t[0.2831,0.2889]  \t[0.2907,0.3040]  \t[0.2800,0.2841]  \t[0.2905,0.3001]  \t[0.1752,0.1802]  \t[0.1752,0.1802]  \t[0.0644,0.0665]  \t[0.0487,0.0534]  \t12.67898  \n",
      "sampler.palgo = tensor([0.0482, 0.1079, 0.4533, 0.1715, 0.2192])\n",
      "0.2: [0.17201818 0.17665744] 0.5: [0.28082843 0.2843356 ]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-housing-expectile.*\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.06449867601748151, 5.978160850480319, 0.110258571535451, 8, 4096, 5\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=True))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9a9b95cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.5)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.1309,0.5237]  \t[0.1309,0.5237]  \t[0.0216,0.0863]  \t[0.0216,0.0863]  \t[0.0000,0.1180]  \t[0.0000,0.1180]  \t[0.3281,1.3125]  \t[0.3281,1.3125]  \t[0.0469,0.1875]  \t[0.0469,0.1875]  \t0.14855   \n",
      "sampler.palgo = tensor([0.1996, 0.1996, 0.2016, 0.1996, 0.1996])\n",
      "2    \t[0.2335,0.5883]  \t[0.1779,0.7115]  \t[0.0654,0.1685]  \t[0.0660,0.2642]  \t[0.0294,0.2051]  \t[0.0587,0.2927]  \t[0.4141,1.0867]  \t[0.4141,1.0867]  \t[0.0703,0.1797]  \t[0.0469,0.1875]  \t0.34347   \n",
      "sampler.palgo = tensor([0.1984, 0.2004, 0.2004, 0.1984, 0.2025])\n",
      "3    \t[0.2534,0.5428]  \t[0.1463,0.5854]  \t[0.0764,0.1571]  \t[0.0420,0.1679]  \t[0.0574,0.1724]  \t[0.0531,0.2238]  \t[0.4943,1.0372]  \t[0.4943,1.0372]  \t[0.0469,0.1198]  \t[0.0000,0.0000]  \t0.54825   \n",
      "sampler.palgo = tensor([0.1976, 0.2016, 0.2016, 0.1976, 0.2017])\n",
      "5    \t[0.3112,0.5353]  \t[0.2472,0.6389]  \t[0.0929,0.1701]  \t[0.0908,0.2223]  \t[0.0961,0.1935]  \t[0.0864,0.2112]  \t[0.5027,0.8144]  \t[0.5027,0.8144]  \t[0.0281,0.0719]  \t[0.0000,0.0000]  \t0.72200   \n",
      "sampler.palgo = tensor([0.1954, 0.2038, 0.2023, 0.1965, 0.2020])\n",
      "9    \t[0.3614,0.5234]  \t[0.3712,0.6242]  \t[0.1220,0.1778]  \t[0.1369,0.2323]  \t[0.1262,0.1944]  \t[0.1492,0.2424]  \t[0.4355,0.6603]  \t[0.4355,0.6603]  \t[0.0486,0.0834]  \t[0.0699,0.1174]  \t0.92522   \n",
      "sampler.palgo = tensor([0.1933, 0.2045, 0.2016, 0.1959, 0.2047])\n",
      "17   \t[0.3975,0.5405]  \t[0.3672,0.5719]  \t[0.1455,0.1983]  \t[0.1488,0.2287]  \t[0.1456,0.2050]  \t[0.1566,0.2381]  \t[0.4695,0.6410]  \t[0.4695,0.6410]  \t[0.0586,0.0856]  \t[0.0546,0.0957]  \t1.18308   \n",
      "sampler.palgo = tensor([0.1902, 0.2059, 0.1984, 0.2001, 0.2053])\n",
      "33   \t[0.4108,0.5232]  \t[0.4133,0.5775]  \t[0.1622,0.2061]  \t[0.1720,0.2450]  \t[0.1603,0.2022]  \t[0.1760,0.2244]  \t[0.4457,0.5563]  \t[0.4457,0.5563]  \t[0.0923,0.1246]  \t[0.1258,0.1759]  \t1.56741   \n",
      "sampler.palgo = tensor([0.1853, 0.2180, 0.1948, 0.2006, 0.2014])\n",
      "65   \t[0.4488,0.5152]  \t[0.4311,0.5450]  \t[0.1846,0.2145]  \t[0.1898,0.2408]  \t[0.1853,0.2158]  \t[0.1894,0.2314]  \t[0.4383,0.5016]  \t[0.4383,0.5016]  \t[0.1094,0.1349]  \t[0.1187,0.1573]  \t2.06682   \n",
      "sampler.palgo = tensor([0.1874, 0.2062, 0.2093, 0.2019, 0.1951])\n",
      "129  \t[0.4622,0.5113]  \t[0.4580,0.5282]  \t[0.2008,0.2225]  \t[0.2057,0.2391]  \t[0.1969,0.2194]  \t[0.2050,0.2348]  \t[0.4268,0.4747]  \t[0.4268,0.4747]  \t[0.1296,0.1470]  \t[0.1423,0.1697]  \t2.82953   \n",
      "sampler.palgo = tensor([0.1822, 0.2043, 0.2043, 0.2293, 0.1798])\n",
      "257  \t[0.4883,0.5237]  \t[0.4824,0.5358]  \t[0.2180,0.2337]  \t[0.2215,0.2450]  \t[0.2104,0.2261]  \t[0.2233,0.2398]  \t[0.3929,0.4220]  \t[0.3929,0.4220]  \t[0.1269,0.1366]  \t[0.1151,0.1320]  \t4.15360   \n",
      "sampler.palgo = tensor([0.1612, 0.1769, 0.2494, 0.2118, 0.2007])\n",
      "513  \t[0.5024,0.5263]  \t[0.5107,0.5467]  \t[0.2382,0.2507]  \t[0.2561,0.2753]  \t[0.2320,0.2433]  \t[0.2553,0.2671]  \t[0.3197,0.3385]  \t[0.3197,0.3385]  \t[0.1106,0.1185]  \t[0.0934,0.1043]  \t6.60584   \n",
      "sampler.palgo = tensor([0.1565, 0.1359, 0.2749, 0.1933, 0.2395])\n",
      "1025 \t[0.5177,0.5337]  \t[0.5247,0.5502]  \t[0.2602,0.2689]  \t[0.2777,0.2912]  \t[0.2562,0.2626]  \t[0.2736,0.2839]  \t[0.2505,0.2604]  \t[0.2505,0.2604]  \t[0.0917,0.0963]  \t[0.0705,0.0757]  \t12.00065  \n",
      "sampler.palgo = tensor([0.1193, 0.1251, 0.2759, 0.1658, 0.3139])\n",
      "2049 \t[0.5281,0.5403]  \t[0.5357,0.5500]  \t[0.2732,0.2801]  \t[0.2856,0.2938]  \t[0.2704,0.2753]  \t[0.2835,0.2900]  \t[0.2064,0.2130]  \t[0.2064,0.2130]  \t[0.0749,0.0774]  \t[0.0567,0.0607]  \t21.53080  \n",
      "sampler.palgo = tensor([0.0815, 0.1468, 0.2532, 0.1879, 0.3307])\n",
      "2519 \t[0.5314,0.5414]  \t[0.5369,0.5615]  \t[0.2771,0.2828]  \t[0.2887,0.3021]  \t[0.2750,0.2790]  \t[0.2872,0.2980]  \t[0.1975,0.2032]  \t[0.1975,0.2032]  \t[0.0708,0.0729]  \t[0.0501,0.0546]  \t25.91788  \n",
      "sampler.palgo = tensor([0.0720, 0.1265, 0.2089, 0.1797, 0.4129])\n",
      "0.2: [0.16284109 0.16747506] 0.5: [0.27435601 0.27891664]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-housing-expectile.*\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.06449867601748151, 5.978160850480319, 0.110258571535451, 8, 4096, 5\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=False))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4645daee",
   "metadata": {},
   "source": [
    "### risk-averse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ca86f9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.2)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.1647,0.6587]  \t[0.1647,0.6587]  \t[0.0211,0.0846]  \t[0.0211,0.0846]  \t[0.0000,0.0588]  \t[0.0000,0.0347]  \t[0.3281,1.3125]  \t[0.3281,1.3125]  \t[0.0469,0.1875]  \t[0.0469,0.1875]  \t0.10310   \n",
      "sampler.palgo = tensor([0.1664, 0.1664, 0.1664, 0.1680, 0.1664, 0.1664])\n",
      "2    \t[0.2168,0.5652]  \t[0.1285,0.5138]  \t[0.0856,0.2329]  \t[0.0970,0.3881]  \t[0.0226,0.1046]  \t[0.0528,0.4122]  \t[0.3359,0.9066]  \t[0.3359,0.9066]  \t[0.0703,0.1797]  \t[0.0469,0.1875]  \t0.27704   \n",
      "sampler.palgo = tensor([0.1652, 0.1652, 0.1666, 0.1682, 0.1666, 0.1681])\n",
      "3    \t[0.2340,0.4941]  \t[0.1153,0.4614]  \t[0.0997,0.2110]  \t[0.0515,0.2059]  \t[0.0247,0.0726]  \t[0.0000,0.0725]  \t[0.4477,0.9279]  \t[0.4477,0.9279]  \t[0.1716,0.3654]  \t[0.1875,0.7500]  \t0.43793   \n",
      "sampler.palgo = tensor([0.1648, 0.1648, 0.1662, 0.1704, 0.1662, 0.1677])\n",
      "5    \t[0.2430,0.4032]  \t[0.1380,0.3556]  \t[0.1208,0.2077]  \t[0.1019,0.2613]  \t[0.0378,0.0853]  \t[0.0628,0.1684]  \t[0.3716,0.6794]  \t[0.3716,0.6794]  \t[0.1280,0.2380]  \t[0.0234,0.0938]  \t0.60842   \n",
      "sampler.palgo = tensor([0.1652, 0.1631, 0.1676, 0.1703, 0.1662, 0.1676])\n",
      "9    \t[0.2023,0.3065]  \t[0.1353,0.2281]  \t[0.1273,0.1872]  \t[0.1171,0.1974]  \t[0.0606,0.1090]  \t[0.0840,0.1473]  \t[0.2464,0.4324]  \t[0.2464,0.4324]  \t[0.1093,0.1980]  \t[0.0781,0.1684]  \t0.81353   \n",
      "sampler.palgo = tensor([0.1653, 0.1616, 0.1662, 0.1729, 0.1652, 0.1689])\n",
      "17   \t[0.1832,0.2545]  \t[0.1340,0.2019]  \t[0.1273,0.1711]  \t[0.1013,0.1574]  \t[0.0759,0.1100]  \t[0.1144,0.1276]  \t[0.1386,0.2328]  \t[0.1386,0.2328]  \t[0.0744,0.1206]  \t[0.0195,0.0430]  \t1.05386   \n",
      "sampler.palgo = tensor([0.1634, 0.1633, 0.1667, 0.1715, 0.1654, 0.1697])\n",
      "33   \t[0.1979,0.2537]  \t[0.2081,0.2848]  \t[0.1444,0.1823]  \t[0.1572,0.2188]  \t[0.0885,0.1112]  \t[0.0861,0.1228]  \t[0.1863,0.2487]  \t[0.1863,0.2487]  \t[0.0829,0.1099]  \t[0.0751,0.1144]  \t1.36732   \n",
      "sampler.palgo = tensor([0.1630, 0.1645, 0.1644, 0.1767, 0.1626, 0.1687])\n",
      "65   \t[0.2153,0.2461]  \t[0.2061,0.2599]  \t[0.1739,0.1994]  \t[0.1845,0.2328]  \t[0.1038,0.1249]  \t[0.1172,0.1456]  \t[0.1891,0.2293]  \t[0.1891,0.2293]  \t[0.1018,0.1262]  \t[0.1143,0.1588]  \t1.73863   \n",
      "sampler.palgo = tensor([0.1568, 0.1668, 0.1739, 0.1773, 0.1558, 0.1693])\n",
      "129  \t[0.2260,0.2511]  \t[0.2310,0.2647]  \t[0.1987,0.2216]  \t[0.2169,0.2506]  \t[0.1188,0.1339]  \t[0.1264,0.1512]  \t[0.2035,0.2320]  \t[0.2035,0.2320]  \t[0.1150,0.1342]  \t[0.1213,0.1448]  \t2.22522   \n",
      "sampler.palgo = tensor([0.1554, 0.1565, 0.1741, 0.1757, 0.1611, 0.1770])\n",
      "257  \t[0.2356,0.2530]  \t[0.2302,0.2559]  \t[0.2233,0.2405]  \t[0.2321,0.2597]  \t[0.1388,0.1495]  \t[0.1527,0.1738]  \t[0.1750,0.1921]  \t[0.1750,0.1921]  \t[0.1097,0.1223]  \t[0.1002,0.1140]  \t2.93089   \n",
      "sampler.palgo = tensor([0.1466, 0.1401, 0.1838, 0.1947, 0.1726, 0.1622])\n",
      "513  \t[0.2347,0.2463]  \t[0.2315,0.2483]  \t[0.2397,0.2517]  \t[0.2532,0.2721]  \t[0.1575,0.1651]  \t[0.1737,0.1843]  \t[0.1418,0.1514]  \t[0.1418,0.1514]  \t[0.1011,0.1095]  \t[0.0897,0.1003]  \t4.13984   \n",
      "sampler.palgo = tensor([0.1543, 0.1491, 0.1706, 0.2064, 0.1532, 0.1664])\n",
      "1025 \t[0.2311,0.2380]  \t[0.2232,0.2340]  \t[0.2520,0.2597]  \t[0.2598,0.2726]  \t[0.1724,0.1779]  \t[0.1843,0.1900]  \t[0.1030,0.1076]  \t[0.1030,0.1076]  \t[0.0876,0.0917]  \t[0.0717,0.0774]  \t6.29457   \n",
      "sampler.palgo = tensor([0.1819, 0.1342, 0.1412, 0.2293, 0.1293, 0.1841])\n",
      "2049 \t[0.2336,0.2384]  \t[0.2347,0.2412]  \t[0.2633,0.2695]  \t[0.2737,0.2810]  \t[0.1808,0.1850]  \t[0.1892,0.1941]  \t[0.0930,0.0959]  \t[0.0930,0.0959]  \t[0.0870,0.0897]  \t[0.0847,0.0892]  \t10.31432  \n",
      "sampler.palgo = tensor([0.1216, 0.2435, 0.1734, 0.1665, 0.1187, 0.1763])\n",
      "2519 \t[0.2331,0.2377]  \t[0.2268,0.2372]  \t[0.2649,0.2704]  \t[0.2680,0.2811]  \t[0.1829,0.1861]  \t[0.1875,0.1943]  \t[0.0878,0.0903]  \t[0.0878,0.0903]  \t[0.0850,0.0881]  \t[0.0770,0.0821]  \t12.36872  \n",
      "sampler.palgo = tensor([0.1403, 0.1934, 0.1786, 0.1679, 0.0981, 0.2218])\n",
      "0.2: [0.18249884 0.18650764] 0.5: [0.26314418 0.26639171]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-housing-expectile.*\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.025544677278674932, 6.554977623553996, 0.12773567975570957, 32, 128, 6\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=True))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9696621b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.2)       \tsince last       \tpnosale          \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.1652,0.6607]  \t[0.1652,0.6607]  \t[0.0216,0.0863]  \t[0.0216,0.0863]  \t[0.0000,0.0601]  \t[0.0000,0.0601]  \t[0.3281,1.3125]  \t[0.3281,1.3125]  \t[0.0000,0.0000]  \t[0.0000,0.0000]  \t0.16172   \n",
      "sampler.palgo = tensor([0.1664, 0.1664, 0.1664, 0.1680, 0.1664, 0.1664])\n",
      "2    \t[0.2321,0.5989]  \t[0.1464,0.5857]  \t[0.0648,0.1663]  \t[0.0650,0.2601]  \t[0.0159,0.0603]  \t[0.0160,0.0948]  \t[0.4141,1.0867]  \t[0.4141,1.0867]  \t[0.0625,0.1883]  \t[0.0938,0.3750]  \t0.29323   \n",
      "sampler.palgo = tensor([0.1656, 0.1672, 0.1656, 0.1672, 0.1656, 0.1689])\n",
      "3    \t[0.2638,0.5497]  \t[0.1409,0.5635]  \t[0.0751,0.1545]  \t[0.0408,0.1632]  \t[0.0151,0.0599]  \t[0.0000,0.0872]  \t[0.4943,1.0372]  \t[0.4943,1.0372]  \t[0.0417,0.1255]  \t[0.0000,0.0000]  \t0.46913   \n",
      "sampler.palgo = tensor([0.1651, 0.1682, 0.1651, 0.1682, 0.1651, 0.1683])\n",
      "5    \t[0.2738,0.4571]  \t[0.1662,0.4333]  \t[0.0913,0.1676]  \t[0.0895,0.2194]  \t[0.0240,0.0601]  \t[0.0255,0.1028]  \t[0.5161,0.8437]  \t[0.5161,0.8437]  \t[0.0625,0.1375]  \t[0.0625,0.1883]  \t0.67582   \n",
      "sampler.palgo = tensor([0.1636, 0.1706, 0.1650, 0.1679, 0.1636, 0.1693])\n",
      "9    \t[0.2391,0.3565]  \t[0.1663,0.2812]  \t[0.1231,0.1805]  \t[0.1400,0.2368]  \t[0.0477,0.0795]  \t[0.0936,0.1644]  \t[0.3460,0.5524]  \t[0.3460,0.5524]  \t[0.0485,0.0921]  \t[0.0156,0.0471]  \t0.92658   \n",
      "sampler.palgo = tensor([0.1616, 0.1700, 0.1646, 0.1676, 0.1645, 0.1717])\n",
      "17   \t[0.2111,0.2912]  \t[0.1479,0.2241]  \t[0.1303,0.1800]  \t[0.1177,0.1812]  \t[0.0740,0.1027]  \t[0.1262,0.1497]  \t[0.1951,0.3011]  \t[0.1951,0.3011]  \t[0.0468,0.0764]  \t[0.0292,0.0605]  \t1.20101   \n",
      "sampler.palgo = tensor([0.1614, 0.1692, 0.1633, 0.1680, 0.1669, 0.1713])\n",
      "33   \t[0.2100,0.2690]  \t[0.2025,0.2728]  \t[0.1475,0.1896]  \t[0.1629,0.2265]  \t[0.0854,0.1076]  \t[0.0849,0.1248]  \t[0.2209,0.2942]  \t[0.2209,0.2942]  \t[0.0818,0.1132]  \t[0.1152,0.1720]  \t1.59918   \n",
      "sampler.palgo = tensor([0.1595, 0.1749, 0.1617, 0.1719, 0.1664, 0.1655])\n",
      "65   \t[0.2254,0.2586]  \t[0.2151,0.2724]  \t[0.1782,0.2051]  \t[0.1892,0.2392]  \t[0.1038,0.1246]  \t[0.1166,0.1502]  \t[0.2132,0.2564]  \t[0.2132,0.2564]  \t[0.0978,0.1212]  \t[0.1001,0.1316]  \t2.18364   \n",
      "sampler.palgo = tensor([0.1632, 0.1591, 0.1620, 0.1833, 0.1745, 0.1579])\n",
      "129  \t[0.2330,0.2591]  \t[0.2327,0.2664]  \t[0.2007,0.2237]  \t[0.2185,0.2500]  \t[0.1166,0.1312]  \t[0.1223,0.1510]  \t[0.2173,0.2468]  \t[0.2173,0.2468]  \t[0.1130,0.1318]  \t[0.1271,0.1480]  \t3.01766   \n",
      "sampler.palgo = tensor([0.1658, 0.1505, 0.1586, 0.1995, 0.1727, 0.1529])\n",
      "257  \t[0.2464,0.2649]  \t[0.2438,0.2711]  \t[0.2269,0.2436]  \t[0.2360,0.2642]  \t[0.1322,0.1444]  \t[0.1423,0.1639]  \t[0.2109,0.2301]  \t[0.2109,0.2301]  \t[0.1211,0.1318]  \t[0.1201,0.1385]  \t4.40955   \n",
      "sampler.palgo = tensor([0.1600, 0.1362, 0.1741, 0.1923, 0.1639, 0.1736])\n",
      "513  \t[0.2490,0.2615]  \t[0.2491,0.2669]  \t[0.2424,0.2550]  \t[0.2552,0.2739]  \t[0.1465,0.1561]  \t[0.1572,0.1695]  \t[0.1899,0.2045]  \t[0.1899,0.2045]  \t[0.1165,0.1235]  \t[0.1091,0.1199]  \t6.87966   \n",
      "sampler.palgo = tensor([0.1686, 0.1169, 0.1775, 0.1662, 0.2006, 0.1702])\n",
      "1025 \t[0.2433,0.2502]  \t[0.2323,0.2437]  \t[0.2557,0.2637]  \t[0.2646,0.2775]  \t[0.1659,0.1718]  \t[0.1835,0.1909]  \t[0.1394,0.1460]  \t[0.1394,0.1460]  \t[0.0991,0.1039]  \t[0.0813,0.0866]  \t11.61640  \n",
      "sampler.palgo = tensor([0.1649, 0.1078, 0.1498, 0.1958, 0.2239, 0.1578])\n",
      "2049 \t[0.2356,0.2405]  \t[0.2266,0.2327]  \t[0.2623,0.2683]  \t[0.2677,0.2748]  \t[0.1757,0.1795]  \t[0.1833,0.1888]  \t[0.1110,0.1149]  \t[0.1110,0.1149]  \t[0.0898,0.0924]  \t[0.0782,0.0827]  \t21.03640  \n",
      "sampler.palgo = tensor([0.1560, 0.1244, 0.1460, 0.1860, 0.2286, 0.1589])\n",
      "2519 \t[0.2351,0.2396]  \t[0.2291,0.2393]  \t[0.2647,0.2703]  \t[0.2720,0.2851]  \t[0.1792,0.1826]  \t[0.1896,0.1970]  \t[0.1036,0.1066]  \t[0.1036,0.1066]  \t[0.0872,0.0895]  \t[0.0738,0.0806]  \t25.81240  \n",
      "sampler.palgo = tensor([0.1435, 0.1118, 0.1472, 0.1766, 0.2283, 0.1926])\n",
      "0.2: [0.17924588 0.18254981] 0.5: [0.26334541 0.2664894 ]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-housing-expectile.*\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.025544677278674932, 6.554977623553996, 0.12773567975570957, 32, 128, 6\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', approxargmax=False))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f1f644",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
