{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76fc1416",
   "metadata": {},
   "source": [
    "Datasets:\n",
    "\n",
    "* [dc2012.arff](https://www.openml.org/search?type=data&status=active&id=42712): already in ARFF format\n",
    "* [londonbike.zip](https://www.kaggle.com/datasets/hmavrodiev/london-bike-sharing-dataset): convert to ARFF with routines in EDA section\n",
    "* [chicagodivvy.zip](https://www.kaggle.com/datasets/yingwurenjian/chicago-divvy-bicycle-sharing-data): convert to ARFF with routines in EDA section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad2090e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))\n",
    "display(\"text/html\", \"<style>.container { width:100% !important; }</style>\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353096ec",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5f963ff",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def flass(wut):\n",
    "    import arff\n",
    "    import pandas as pd\n",
    "    data = arff.load(open(wut, 'r'))\n",
    "    z = pd.DataFrame(data['data'])\n",
    "    z.columns = [ v[0].lower() for v in data['attributes'] ]\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926227a9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3214b83a",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def loadchicago():\n",
    "    import arff\n",
    "    import pandas as pd\n",
    "    import zipfile\n",
    "    \n",
    "    goodcols = ['year','month', 'week', 'day', 'hour', 'temperature', 'events']\n",
    "    \n",
    "    with zipfile.ZipFile('chicagodivvy.zip') as myzip:\n",
    "        with myzip.open('data.csv') as myfile:\n",
    "            z = pd.read_csv(myfile)\n",
    "            return z[goodcols].groupby(goodcols).size().to_frame(name='count').reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75212851",
   "metadata": {
    "code_folding": [
     0,
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convertchicago(df):\n",
    "    import arff\n",
    "\n",
    "    def converttype(dtype):\n",
    "        if dtype == 'int64':\n",
    "            return 'INTEGER'\n",
    "        elif dtype == 'float64':\n",
    "            return 'REAL'\n",
    "        elif dtype == 'O':\n",
    "            return 'STRING'\n",
    "        else:\n",
    "            assert False\n",
    "    \n",
    "    rename = { \n",
    "        'cnt': 'count',\n",
    "    }\n",
    "    attributes = [(rename.get(c, c), converttype(df[j].dtypes)) for j, c in zip(df, df.columns.values) ]\n",
    "    arff_dic = {\n",
    "        'attributes': attributes,\n",
    "        'data': df.values,\n",
    "        'relation': 'myRel',\n",
    "        'description': ''\n",
    "    }\n",
    "\n",
    "    with open(\"chicago.arff\", \"w\", encoding=\"utf8\") as f:\n",
    "         arff.dump(arff_dic, f)\n",
    "            \n",
    "convertchicago(loadchicago())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a433662c",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "      <th>hour</th>\n",
       "      <th>temperature</th>\n",
       "      <th>events</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10.9</td>\n",
       "      <td>not clear</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>not clear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>14.0</td>\n",
       "      <td>not clear</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>15.1</td>\n",
       "      <td>rain or snow</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2014</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>17.1</td>\n",
       "      <td>rain or snow</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34612</th>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>17</td>\n",
       "      <td>7.0</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34613</th>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>18</td>\n",
       "      <td>5.0</td>\n",
       "      <td>cloudy</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34614</th>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>20</td>\n",
       "      <td>1.9</td>\n",
       "      <td>clear</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34615</th>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>clear</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34616</th>\n",
       "      <td>2017</td>\n",
       "      <td>12</td>\n",
       "      <td>52</td>\n",
       "      <td>6</td>\n",
       "      <td>23</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>clear</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>34617 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       year  month  week  day  hour  temperature        events  count\n",
       "0      2014      1     1    2     1         10.9     not clear     10\n",
       "1      2014      1     1    2     2         12.0     not clear      2\n",
       "2      2014      1     1    2     3         14.0     not clear      3\n",
       "3      2014      1     1    2     6         15.1  rain or snow      1\n",
       "4      2014      1     1    2     7         17.1  rain or snow      4\n",
       "...     ...    ...   ...  ...   ...          ...           ...    ...\n",
       "34612  2017     12    52    6    17          7.0        cloudy     38\n",
       "34613  2017     12    52    6    18          5.0        cloudy     27\n",
       "34614  2017     12    52    6    20          1.9         clear     19\n",
       "34615  2017     12    52    6    22         -0.0         clear      9\n",
       "34616  2017     12    52    6    23         -0.9         clear      7\n",
       "\n",
       "[34617 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flass('chicago.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e947ae9",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b9c4675",
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def loadlondon():\n",
    "    import arff\n",
    "    import pandas as pd\n",
    "    import zipfile    \n",
    "    with zipfile.ZipFile('londonbike.zip') as myzip:\n",
    "        with myzip.open('london_merged.csv') as myfile:\n",
    "            z = pd.read_csv(myfile)\n",
    "            return z\n",
    "        \n",
    "loadlondon()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a408d7",
   "metadata": {
    "code_folding": [
     0,
     3
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def convertlondon(df):\n",
    "    import arff\n",
    "\n",
    "    def converttype(dtype):\n",
    "        if dtype == 'int64':\n",
    "            return 'INTEGER'\n",
    "        elif dtype == 'float64':\n",
    "            return 'REAL'\n",
    "        elif dtype == 'O':\n",
    "            return 'STRING'\n",
    "        else:\n",
    "            assert False\n",
    "    \n",
    "    rename = { \n",
    "        'cnt': 'count',\n",
    "    }\n",
    "    attributes = [(rename.get(c, c), converttype(df[j].dtypes)) for j, c in zip(df, df.columns.values) ]\n",
    "    arff_dic = {\n",
    "        'attributes': attributes,\n",
    "        'data': df.values,\n",
    "        'relation': 'myRel',\n",
    "        'description': ''\n",
    "    }\n",
    "\n",
    "    with open(\"london.arff\", \"w\", encoding=\"utf8\") as f:\n",
    "         arff.dump(arff_dic, f)\n",
    "            \n",
    "convertlondon(loadlondon())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8be10d28",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>count</th>\n",
       "      <th>t1</th>\n",
       "      <th>t2</th>\n",
       "      <th>hum</th>\n",
       "      <th>wind_speed</th>\n",
       "      <th>weather_code</th>\n",
       "      <th>is_holiday</th>\n",
       "      <th>is_weekend</th>\n",
       "      <th>season</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2015-01-04 00:00:00</td>\n",
       "      <td>182</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2015-01-04 01:00:00</td>\n",
       "      <td>138</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>93.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2015-01-04 02:00:00</td>\n",
       "      <td>134</td>\n",
       "      <td>2.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>96.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2015-01-04 03:00:00</td>\n",
       "      <td>72</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2015-01-04 04:00:00</td>\n",
       "      <td>47</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17409</th>\n",
       "      <td>2017-01-03 19:00:00</td>\n",
       "      <td>1042</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>19.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17410</th>\n",
       "      <td>2017-01-03 20:00:00</td>\n",
       "      <td>541</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>21.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17411</th>\n",
       "      <td>2017-01-03 21:00:00</td>\n",
       "      <td>337</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>78.5</td>\n",
       "      <td>24.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17412</th>\n",
       "      <td>2017-01-03 22:00:00</td>\n",
       "      <td>224</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>76.0</td>\n",
       "      <td>23.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17413</th>\n",
       "      <td>2017-01-03 23:00:00</td>\n",
       "      <td>139</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>22.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17414 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 timestamp  count   t1   t2    hum  wind_speed  weather_code  \\\n",
       "0      2015-01-04 00:00:00    182  3.0  2.0   93.0         6.0           3.0   \n",
       "1      2015-01-04 01:00:00    138  3.0  2.5   93.0         5.0           1.0   \n",
       "2      2015-01-04 02:00:00    134  2.5  2.5   96.5         0.0           1.0   \n",
       "3      2015-01-04 03:00:00     72  2.0  2.0  100.0         0.0           1.0   \n",
       "4      2015-01-04 04:00:00     47  2.0  0.0   93.0         6.5           1.0   \n",
       "...                    ...    ...  ...  ...    ...         ...           ...   \n",
       "17409  2017-01-03 19:00:00   1042  5.0  1.0   81.0        19.0           3.0   \n",
       "17410  2017-01-03 20:00:00    541  5.0  1.0   81.0        21.0           4.0   \n",
       "17411  2017-01-03 21:00:00    337  5.5  1.5   78.5        24.0           4.0   \n",
       "17412  2017-01-03 22:00:00    224  5.5  1.5   76.0        23.0           4.0   \n",
       "17413  2017-01-03 23:00:00    139  5.0  1.0   76.0        22.0           2.0   \n",
       "\n",
       "       is_holiday  is_weekend  season  \n",
       "0             0.0         1.0     3.0  \n",
       "1             0.0         1.0     3.0  \n",
       "2             0.0         1.0     3.0  \n",
       "3             0.0         1.0     3.0  \n",
       "4             0.0         1.0     3.0  \n",
       "...           ...         ...     ...  \n",
       "17409         0.0         0.0     3.0  \n",
       "17410         0.0         0.0     3.0  \n",
       "17411         0.0         0.0     3.0  \n",
       "17412         0.0         0.0     3.0  \n",
       "17413         0.0         0.0     3.0  \n",
       "\n",
       "[17414 rows x 10 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flass('london.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23561f0a",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "## Bike Sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "3208a67b",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>hour</th>\n",
       "      <th>holiday</th>\n",
       "      <th>weekday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>feel_temp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>clear</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>0.81</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>clear</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>clear</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>0.80</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>clear</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>spring</td>\n",
       "      <td>2011</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>clear</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>0.75</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17374</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>19</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>misty</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>11</td>\n",
       "      <td>108</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17375</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>misty</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>8</td>\n",
       "      <td>81</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17376</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>0.60</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>7</td>\n",
       "      <td>83</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17377</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>0.56</td>\n",
       "      <td>8.9981</td>\n",
       "      <td>13</td>\n",
       "      <td>48</td>\n",
       "      <td>61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17378</th>\n",
       "      <td>spring</td>\n",
       "      <td>2012</td>\n",
       "      <td>12</td>\n",
       "      <td>23</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>clear</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>0.65</td>\n",
       "      <td>8.9981</td>\n",
       "      <td>12</td>\n",
       "      <td>37</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>17379 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       season  year  month  hour  holiday  weekday  workingday weather   temp  \\\n",
       "0      spring  2011      1     0        0        6           0   clear   9.84   \n",
       "1      spring  2011      1     1        0        6           0   clear   9.02   \n",
       "2      spring  2011      1     2        0        6           0   clear   9.02   \n",
       "3      spring  2011      1     3        0        6           0   clear   9.84   \n",
       "4      spring  2011      1     4        0        6           0   clear   9.84   \n",
       "...       ...   ...    ...   ...      ...      ...         ...     ...    ...   \n",
       "17374  spring  2012     12    19        0        1           1   misty  10.66   \n",
       "17375  spring  2012     12    20        0        1           1   misty  10.66   \n",
       "17376  spring  2012     12    21        0        1           1   clear  10.66   \n",
       "17377  spring  2012     12    22        0        1           1   clear  10.66   \n",
       "17378  spring  2012     12    23        0        1           1   clear  10.66   \n",
       "\n",
       "       feel_temp  humidity  windspeed  casual  registered  count  \n",
       "0         14.395      0.81     0.0000       3          13     16  \n",
       "1         13.635      0.80     0.0000       8          32     40  \n",
       "2         13.635      0.80     0.0000       5          27     32  \n",
       "3         14.395      0.75     0.0000       3          10     13  \n",
       "4         14.395      0.75     0.0000       0           1      1  \n",
       "...          ...       ...        ...     ...         ...    ...  \n",
       "17374     12.880      0.60    11.0014      11         108    119  \n",
       "17375     12.880      0.60    11.0014       8          81     89  \n",
       "17376     12.880      0.60    11.0014       7          83     90  \n",
       "17377     13.635      0.56     8.9981      13          48     61  \n",
       "17378     13.635      0.65     8.9981      12          37     49  \n",
       "\n",
       "[17379 rows x 15 columns]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flass('bikesharingdemand.arff')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4381a8",
   "metadata": {},
   "source": [
    "# Arff2Pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e68df6d",
   "metadata": {
    "code_folding": [
     2,
     35,
     36,
     46,
     56,
     59,
     65,
     69,
     70,
     122,
     165
    ]
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EasyAcc:\n",
    "    def __init__(self):\n",
    "        self.n = 0\n",
    "        self.sum = 0\n",
    "        self.sumsq = 0\n",
    "\n",
    "    def __iadd__(self, other):\n",
    "        import math\n",
    "        if not math.isnan(other):\n",
    "            self.n += 1\n",
    "            self.sum += other\n",
    "            self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def __isub__(self, other):\n",
    "        import math\n",
    "        if not math.isnan(other):\n",
    "            self.n += 1\n",
    "            self.sum -= other\n",
    "            self.sumsq += other*other\n",
    "        return self\n",
    "\n",
    "    def mean(self):\n",
    "        return self.sum / max(self.n, 1)\n",
    "\n",
    "    def var(self):\n",
    "        from math import sqrt\n",
    "        return sqrt(self.sumsq / max(self.n, 1) - self.mean()**2)\n",
    "\n",
    "    def semean(self):\n",
    "        from math import sqrt\n",
    "        return self.var() / sqrt(max(self.n, 1))\n",
    "\n",
    "class EasyPoissonBootstrapAcc:\n",
    "    def __init__(self, batch_size, confidence=0.95, seed=2112):\n",
    "        from math import ceil\n",
    "        from numpy.random import default_rng\n",
    "        \n",
    "        self.n = 0\n",
    "        self.batch_size = batch_size\n",
    "        self.confidence = confidence\n",
    "        self.samples = [ EasyAcc() for _ in range(int(ceil(3 / (1 - self.confidence)))) ]\n",
    "        self.rng = default_rng(seed)\n",
    "        \n",
    "    def __iadd__(self, other):\n",
    "        self.n += 1\n",
    "        \n",
    "        poissons = self.rng.poisson(lam=self.batch_size, size=len(self.samples)) / self.batch_size\n",
    "        \n",
    "        for n, (chirp, acc) in enumerate(zip(poissons, self.samples)):\n",
    "            acc += (chirp if n > 0 else 1) * other\n",
    "            \n",
    "        return self\n",
    "         \n",
    "    def __isub__(self, other):\n",
    "        return self.__iadd__(-other)\n",
    "    \n",
    "    def ci(self):\n",
    "        import numpy\n",
    "        quantiles = numpy.quantile(a=[ x.mean() for x in self.samples ],\n",
    "                                   q=[1 - self.confidence, 0.5, self.confidence])\n",
    "        return list(quantiles)\n",
    "    \n",
    "    def formatci(self):\n",
    "        z = self.ci()\n",
    "        return '[{:<.4f},{:<.4f}]'.format(z[0], z[2])\n",
    "    \n",
    "class Schema(object):\n",
    "    def __init__(self, *, attributes, target, skipcol, data):\n",
    "        super().__init__()\n",
    "        \n",
    "        schema = {}\n",
    "        n = 0\n",
    "        for kraw, v in attributes:\n",
    "            k = kraw.lower()\n",
    "\n",
    "            if k in skipcol:\n",
    "                continue\n",
    "                \n",
    "            if isinstance(v, str):\n",
    "                if v in ['INTEGER', 'REAL']:\n",
    "                    if any(thisv is None for row in data for thisk, thisv in zip(attributes, row) if thisk[0].lower() == k):\n",
    "                        assert k != target, (k, target)\n",
    "                        schema[k] = (lambda i: (lambda z: (i+1, 1) if z is None else (i, z)))(n)\n",
    "                        n += 2\n",
    "                    else:\n",
    "                        schema[k] = (lambda i: (lambda z: (i, z)))(n)\n",
    "                        n += 1\n",
    "                elif k == 'date' or k == 'timestamp':\n",
    "                    import ciso8601\n",
    "                    import time\n",
    "                    \n",
    "                    schema[k] = (lambda i: (lambda z: (i, time.mktime(ciso8601.parse_datetime(z).timetuple()))))(n)\n",
    "                    n += 1\n",
    "                elif v == 'STRING':\n",
    "                    uniques = set([ thisv for row in data for thisk, thisv in zip(attributes, row) if thisk[0].lower() == k ])\n",
    "                    schema[k] =  (lambda h: (lambda z: (h[z], 1)))({ z: (n + m) for m, z in enumerate(uniques) })\n",
    "                    n += len(uniques)\n",
    "                else:\n",
    "                    assert False, (k, v)\n",
    "            elif isinstance(v, list) and all((isinstance(z, str) for z in v)):\n",
    "                assert k != target, (k, target)\n",
    "                schema[k] = (lambda h: (lambda z: (h[z], 1)))({ z: (n + m) for m, z in enumerate(v) })\n",
    "                n += len(v)\n",
    "            else:\n",
    "                assert False\n",
    "                \n",
    "            if k == target:\n",
    "                n -= 1\n",
    "                \n",
    "        assert target in schema, (target, attributes)\n",
    "                \n",
    "        self.schema = schema\n",
    "        self.target = target\n",
    "        self.nfeatures = n \n",
    "        \n",
    "    def featurize(self, colname, val):\n",
    "        if colname in self.schema:\n",
    "            yield self.schema[colname](val)\n",
    "\n",
    "def makeData(filename, *, target, skipcol, skiprow):\n",
    "    import arff\n",
    "    import numpy\n",
    "    \n",
    "    data = arff.load(open(filename, 'r'))\n",
    "    schema = Schema(attributes=data['attributes'], target=target, skipcol=skipcol, data=data['data'])\n",
    "    \n",
    "    Y = []\n",
    "    X = []\n",
    "    \n",
    "    for row in data['data']:\n",
    "        hashrow = { kraw[0].lower(): v for kraw, v in zip(data['attributes'], row) }\n",
    "        \n",
    "        if skiprow(hashrow):\n",
    "            continue\n",
    "        \n",
    "        y = None\n",
    "        x = [0]*schema.nfeatures\n",
    "        for col, val in hashrow.items():\n",
    "            if col == target:\n",
    "                y = next(schema.featurize(col, val))[1]\n",
    "            else:\n",
    "                for f, vf in schema.featurize(col, val):\n",
    "                    from numbers import Number\n",
    "                    assert isinstance(vf, Number), (col, val, f, vf)\n",
    "                    x[f] = vf\n",
    "                    \n",
    "        Y.append(y)\n",
    "        X.append(x)\n",
    "\n",
    "    Y = numpy.array(Y)\n",
    "    Ymin, Ymax = numpy.min(Y), numpy.max(Y)\n",
    "    Y = (Y - Ymin) / (Ymax - Ymin)\n",
    "    X = numpy.array(X)\n",
    "    Xmin, Xmax = numpy.min(X, axis=0, keepdims=True), numpy.max(X, axis=0, keepdims=True)\n",
    "    if numpy.any(Xmin >= Xmax):\n",
    "        X = X[:,Xmin[0,:] < Xmax[0,:]]\n",
    "        Xmin, Xmax = numpy.min(X, axis=0, keepdims=True), numpy.max(X, axis=0, keepdims=True)\n",
    "    assert numpy.all(Xmax > Xmin), [ (col, lb, ub) for col, (lb, ub) in enumerate(zip(Xmin[0,:], Xmax[0,:])) if lb >= ub ]\n",
    "    X = (X - Xmin) / (Xmax - Xmin)\n",
    "    \n",
    "    return X, Y\n",
    "\n",
    "class ArffToPytorch(torch.utils.data.Dataset):\n",
    "    def __init__(self, filename, *, target, skipcol=[], skiprow=(lambda z: False)):\n",
    "        X, Y = makeData(filename, target=target, skipcol=skipcol, skiprow=skiprow)\n",
    "        self.Xs = torch.Tensor(X)\n",
    "        self.Ys = torch.Tensor(Y).unsqueeze(1)\n",
    "            \n",
    "    def __len__(self):\n",
    "        return self.Xs.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Select sample\n",
    "        return self.Xs[index], self.Ys[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48462203",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "89fdff52",
   "metadata": {
    "code_folding": [
     3
    ],
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_predict': 0.14446721971035004,\n",
       " 'best_constant_loss_predict': 0.14095699787139893,\n",
       " 'best_constant_average_reward': 0.8590430021286011,\n",
       " 'best_constant_average_logloss': 0.4066221208366401}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best constant predictor\n",
    "# if you don't beat this, you have a problem\n",
    "\n",
    "def bestconstant(dataset):\n",
    "    import numpy\n",
    "        \n",
    "    ymed = torch.median(dataset.Ys).item()\n",
    "    ymedabsloss = torch.mean(torch.abs(dataset.Ys - ymed)).item()\n",
    "\n",
    "    l1_loss = torch.nn.L1Loss(reduction='none')\n",
    "    log_loss = torch.nn.BCELoss()\n",
    "    sumabsloss, sumlogloss = EasyAcc(), EasyAcc()\n",
    "    \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for _, Ys in generator:\n",
    "        with torch.no_grad():\n",
    "            yhat = torch.Tensor([ymed]).expand(*Ys.shape)\n",
    "            loss = l1_loss(input=yhat, target=Ys)\n",
    "            losspredict = torch.Tensor([ymedabsloss]).expand(*Ys.shape)\n",
    "            loglosspredict = log_loss(input=losspredict, target=loss)\n",
    "            \n",
    "            sumabsloss += torch.mean(losspredict).item()\n",
    "            sumlogloss += torch.mean(loglosspredict).item()\n",
    "    \n",
    "    return { 'best_constant_predict': ymed,\n",
    "             'best_constant_loss_predict': ymedabsloss,\n",
    "             'best_constant_average_reward': 1 - sumabsloss.mean(), \n",
    "             'best_constant_average_logloss': sumlogloss.mean(),\n",
    "           }\n",
    "\n",
    "bestconstant(ArffToPytorch('bikesharingdemand.arff', target='count'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eca68a96",
   "metadata": {
    "code_folding": [
     0,
     1,
     10,
     11,
     25,
     31
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class LinearArgmax(torch.nn.Module):\n",
    "    def __init__(self, dobs):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.linear = torch.nn.Linear(in_features=dobs, out_features=1)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, Xs):\n",
    "        return self.sigmoid(self.linear(Xs))\n",
    "\n",
    "class CauchyRFF(torch.nn.Module):\n",
    "    def __init__(self, dobs, numrff, sigma, device):\n",
    "        from math import pi, sqrt\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rffW = torch.nn.Parameter(torch.empty(dobs, numrff).cauchy_(sigma = sigma).to(device), \n",
    "                                       requires_grad=False)\n",
    "        self.rffb = torch.nn.Parameter((2 * pi * torch.rand(numrff)).to(device),\n",
    "                                       requires_grad=False)\n",
    "        self.sqrtrff = torch.nn.Parameter(torch.Tensor([sqrt(numrff)]).to(device), \n",
    "                                          requires_grad=False)\n",
    "        self.linear = torch.nn.Linear(in_features=numrff, out_features=1, device=device)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, Xs):\n",
    "        with torch.no_grad():\n",
    "            rff = (torch.matmul(Xs, self.rffW) + self.rffb).cos() / self.sqrtrff\n",
    "            \n",
    "        return self.sigmoid(self.linear(rff))\n",
    "    \n",
    "def learnOnline(dataset, *, seed, batch_size, initlr, tzero, modelfactory):\n",
    "    import time\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = None\n",
    "    l1_loss = torch.nn.L1Loss()\n",
    "    \n",
    "    print('{:<5s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}\\t{:<8s}'.format('n', 'loss', 'since last', 'acc', 'acc since last', 'dt (sec)'),\n",
    "          flush=True)\n",
    "    avloss, acc, sincelast, accsincelast = EasyAcc(), EasyAcc(), EasyAcc(), EasyAcc()\n",
    "    \n",
    "    for bno, (Xs, ys) in enumerate(generator):\n",
    "        if model is None:\n",
    "            from math import sqrt\n",
    "            model = modelfactory(Xs.shape[1])\n",
    "            opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: sqrt(tzero) / sqrt(tzero + t))\n",
    "            start = time.time()\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        yhat = model(Xs)\n",
    "        loss = l1_loss(yhat, ys)\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc += torch.mean((torch.abs(yhat - ys) <= 0.1).float())\n",
    "            accsincelast += torch.mean((torch.abs(yhat - ys) <= 0.1).float())\n",
    "            avloss += loss\n",
    "            sincelast += loss\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            print('{:<5d}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}'.format(\n",
    "                    avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "                  flush=True)\n",
    "            sincelast, accsincelast = EasyAcc(), EasyAcc()\n",
    "\n",
    "    print('{:<5d}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}\\t{:<8.5f}'.format(\n",
    "            avloss.n, avloss.mean(), sincelast.mean(), acc.mean(), accsincelast.mean(), time.time() - start), \n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d1196ee",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "mydata = ArffToPytorch('bikesharingdemand.arff', target='count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6aaa22be",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.56430 \t0.56430 \t0.00000 \t0.00000 \t0.17370 \n",
      "2    \t0.34986 \t0.13541 \t0.00000 \t0.00000 \t0.17889 \n",
      "3    \t0.37006 \t0.41048 \t0.00000 \t0.00000 \t0.18068 \n",
      "5    \t0.31444 \t0.23100 \t0.20000 \t0.50000 \t0.18311 \n",
      "9    \t0.32553 \t0.33940 \t0.11111 \t0.00000 \t0.18714 \n",
      "17   \t0.26955 \t0.20657 \t0.11765 \t0.12500 \t0.19272 \n",
      "33   \t0.23044 \t0.18887 \t0.21212 \t0.31250 \t0.20778 \n",
      "65   \t0.20346 \t0.17564 \t0.24615 \t0.28125 \t0.24264 \n",
      "129  \t0.17264 \t0.14134 \t0.33333 \t0.42188 \t0.29994 \n",
      "257  \t0.15892 \t0.14510 \t0.40467 \t0.47656 \t0.39948 \n",
      "513  \t0.13541 \t0.11181 \t0.47953 \t0.55469 \t0.57613 \n",
      "1025 \t0.11255 \t0.08965 \t0.56683 \t0.65430 \t0.82877 \n",
      "2049 \t0.09172 \t0.07087 \t0.66813 \t0.76953 \t1.30280 \n",
      "4097 \t0.07220 \t0.05268 \t0.77715 \t0.88623 \t2.25181 \n",
      "8193 \t0.05589 \t0.03957 \t0.87392 \t0.97070 \t4.28559 \n",
      "16385\t0.04423 \t0.03257 \t0.93555 \t0.99719 \t8.30233 \n",
      "17379\t0.04344 \t0.03037 \t0.93918 \t0.99899 \t8.78822 \n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, seed=4545, batch_size=1, initlr=1e-2, tzero=100, modelfactory=lambda x: LinearArgmax(dobs=x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4d98f34f",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss    \tsince last\tacc     \tacc since last\tdt (sec)\n",
      "1    \t0.49805 \t0.49805 \t0.00000 \t0.00000 \t0.00631 \n",
      "2    \t0.28694 \t0.07582 \t0.50000 \t1.00000 \t0.00924 \n",
      "3    \t0.33834 \t0.44114 \t0.33333 \t0.00000 \t0.01313 \n",
      "5    \t0.28489 \t0.20471 \t0.40000 \t0.50000 \t0.01629 \n",
      "9    \t0.29312 \t0.30341 \t0.22222 \t0.00000 \t0.02419 \n",
      "17   \t0.22303 \t0.14418 \t0.29412 \t0.37500 \t0.03327 \n",
      "33   \t0.17277 \t0.11936 \t0.36364 \t0.43750 \t0.04596 \n",
      "65   \t0.16483 \t0.15665 \t0.46154 \t0.56250 \t0.06657 \n",
      "129  \t0.14459 \t0.12403 \t0.48837 \t0.51562 \t0.11263 \n",
      "257  \t0.12979 \t0.11488 \t0.54864 \t0.60938 \t0.21041 \n",
      "513  \t0.10791 \t0.08593 \t0.64327 \t0.73828 \t0.38694 \n",
      "1025 \t0.08299 \t0.05802 \t0.75415 \t0.86523 \t0.68313 \n",
      "2049 \t0.06517 \t0.04733 \t0.83748 \t0.92090 \t1.33278 \n",
      "4097 \t0.05089 \t0.03661 \t0.89871 \t0.95996 \t2.60740 \n",
      "8193 \t0.03938 \t0.02786 \t0.94019 \t0.98169 \t5.05225 \n",
      "16385\t0.03055 \t0.02171 \t0.96564 \t0.99109 \t10.08077\n",
      "17379\t0.02988 \t0.01889 \t0.96720 \t0.99296 \t10.62600\n"
     ]
    }
   ],
   "source": [
    "learnOnline(mydata, seed=4545, batch_size=1, initlr=1e-2, tzero=100, \n",
    "            modelfactory=lambda x: CauchyRFF(dobs=x, numrff=1024, sigma=1/10, device='cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61bf77d",
   "metadata": {},
   "source": [
    "# Bandit Feedback"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "860526c2",
   "metadata": {},
   "source": [
    "| dataset | $q$ | Avg. Profit | Sold Out (%) | \n",
    "| --- | --- | --- | --- |\n",
    "| DC | 0.2 | [7.82,8.09] | [30.72,31.51] | \n",
    "| DC | 0.5 | [9.88,10.23] | [18.92,19.54] | \n",
    "| London | 0.2 | [3.52,3.63] | [56.35,57.85]  | \n",
    "| London | 0.5 | [4.75,4.95]  | [28.15,29.08]  |\n",
    "| Chicago | 0.2 | [2.17,2.26] | [50.02,50.77] | \n",
    "| Chicago | 0.5 | [3.73,3.83] | [20.01,20.39] |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bb5bb1d8",
   "metadata": {
    "code_folding": [
     4,
     5,
     43,
     63,
     79,
     88,
     112,
     133,
     150,
     161,
     175
    ]
   },
   "outputs": [],
   "source": [
    "def profit(allocated, demand, *, cost, price):\n",
    "    assert price > cost, (price, cost)\n",
    "    return price * torch.clip(demand, max=allocated) - cost * allocated\n",
    "\n",
    "class CauchyTruncatedNormal(torch.nn.Module):\n",
    "    def __init__(self, dobs, numrff, sigma, device, cost, price, approxargmax):\n",
    "        from math import pi, sqrt\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.rffW = torch.nn.Parameter(torch.empty(dobs, numrff).cauchy_(sigma = sigma).to(device), \n",
    "                                       requires_grad=False)\n",
    "        self.rffb = torch.nn.Parameter((2 * pi * torch.rand(numrff)).to(device),\n",
    "                                       requires_grad=False)\n",
    "        self.sqrtrff = torch.nn.Parameter(torch.Tensor([sqrt(numrff)]).to(device), \n",
    "                                          requires_grad=False)\n",
    "        self.linear = torch.nn.Linear(in_features=numrff, out_features=2, device=device)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        self.softplus = torch.nn.Softplus()\n",
    "        self.cost = cost\n",
    "        self.price = price\n",
    "        self.approxargmax = approxargmax\n",
    "        \n",
    "    def distparams(self, Xs):\n",
    "        with torch.no_grad():\n",
    "            rff = (torch.matmul(Xs, self.rffW) + self.rffb).cos() / self.sqrtrff\n",
    "            \n",
    "        pre = self.linear(rff)\n",
    "        return self.sigmoid(pre[:,0:1]), 0.01 + self.softplus(pre[:,1:2])\n",
    "    \n",
    "    def forward(self, Xs, As):\n",
    "        from math import pi, sqrt\n",
    "        \n",
    "        mu, sigma = self.distparams(Xs)\n",
    "        dens = torch.erf((1 - mu) / sigma) + torch.erf(mu / sigma)\n",
    "        nums = (\n",
    "              (sigma / sqrt(pi)) * (torch.exp(-torch.square(mu) / torch.square(sigma)) - torch.exp(-torch.square(mu - As) / torch.square(sigma)))\n",
    "            + As * torch.erf((1 - mu) / sigma)\n",
    "            + (mu - As) * torch.erf((As - mu) / sigma)\n",
    "            + mu * torch.erf(mu / sigma)\n",
    "        )\n",
    "        return -self.cost * As + self.price * nums / dens\n",
    "    \n",
    "    def argmaxone(self, mu, sigma):\n",
    "        from math import erf, exp, pi, sqrt\n",
    "        import scipy.optimize as so\n",
    "        \n",
    "        dens = erf((1 - mu) / sigma) + erf(mu / sigma)\n",
    "        \n",
    "        def fun(z):\n",
    "            nums = (\n",
    "                (sigma / sqrt(pi)) * (exp(-mu**2 / sigma**2) - exp(-(mu - z)**2 / sigma**2))\n",
    "                + z * erf((1 - mu) / sigma)\n",
    "                + (mu - z) * erf((z - mu) / sigma)\n",
    "                + mu * erf(mu / sigma)\n",
    "            )                         \n",
    "            return self.cost * z - self.price * nums / dens\n",
    "        \n",
    "        const = erf((1 - mu) / sigma)\n",
    "        res = so.minimize_scalar(fun=fun, bounds=[0, 1], method='bounded')\n",
    "        assert res.success, res\n",
    "        return res.x\n",
    "\n",
    "    def argmax(self, Xs, resolution):\n",
    "        with torch.no_grad():\n",
    "            if self.approxargmax:\n",
    "                from math import ceil\n",
    "                nsamples = int(ceil(1/resolution))\n",
    "                As = torch.rand(size=(Xs.shape[0], nsamples), device=Xs.device)\n",
    "                weirdrv = torch.max(input=self.forward(Xs, As), dim=1, keepdim=True)\n",
    "                fhatstar = weirdrv.values\n",
    "                ahatstar = torch.gather(input=As, dim=1, index=weirdrv.indices)\n",
    "                return fhatstar, ahatstar\n",
    "            else:\n",
    "                mu, sigma = self.distparams(Xs)\n",
    "                ahatstar = torch.Tensor([ [ self.argmaxone(m.item(), s.item()) ] for m, s in zip(mu, sigma) ])\n",
    "                return self.forward(Xs, ahatstar), ahatstar\n",
    "\n",
    "class CorralIGW(object):\n",
    "    def __init__(self, *, eta, gammamin, gammamax, nalgos, device):\n",
    "        import numpy\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.eta = eta / nalgos\n",
    "        self.gammas = torch.Tensor(numpy.geomspace(gammamin, gammamax, nalgos)).to(device)\n",
    "        self.invpalgo = torch.Tensor([ self.gammas.shape[0] ] * self.gammas.shape[0]).to(device)\n",
    "        \n",
    "    def update(self, algo, invprop, reward):\n",
    "        import numpy\n",
    "        from scipy import optimize\n",
    "                \n",
    "        weightedlosses = self.eta * (-reward.squeeze(1)) * invprop.squeeze(1)\n",
    "        newinvpalgo = torch.scatter(input=self.invpalgo,\n",
    "                                    dim=0,\n",
    "                                    index=algo,\n",
    "                                    src=weightedlosses,\n",
    "                                    reduce='add')\n",
    "                                    \n",
    "        # just do this calc on the cpu\n",
    "        invp = newinvpalgo.cpu().numpy() \n",
    "        invp += 1 - numpy.min(invp)\n",
    "        Zlb = 0\n",
    "        Zub = 1\n",
    "        while (numpy.sum(1 / (invp + Zub)) > 1):\n",
    "            Zlb = Zub\n",
    "            Zub *= 2 \n",
    "        root, res = optimize.brentq(lambda z: 1 - numpy.sum(1 / (invp + z)), Zlb, Zub, full_output=True)\n",
    "        assert res.converged, res\n",
    "        \n",
    "        self.invpalgo = torch.Tensor(invp + root, device=self.invpalgo.device)\n",
    "\n",
    "    def sample(self, fhat, X):\n",
    "        N, _ = X.shape\n",
    "\n",
    "        algosampler = torch.distributions.categorical.Categorical(probs=1.0/self.invpalgo, validate_args=False)\n",
    "        algo = algosampler.sample((N,))\n",
    "        invpalgo = torch.gather(input=self.invpalgo.unsqueeze(0).expand(N, -1),\n",
    "                                dim=1,\n",
    "                                index=algo.unsqueeze(1))\n",
    "        gamma = torch.gather(input=self.gammas.unsqueeze(0).expand(N, -1),\n",
    "                             dim=1,\n",
    "                             index=algo.unsqueeze(1))\n",
    "        resolution = 1 / torch.max(gamma).item()\n",
    "        fhatstar, ahatstar = fhat.argmax(X, resolution)\n",
    "        \n",
    "        rando = torch.rand(size=(N, 1), device=X.device)\n",
    "        fhatrando = fhat(X, rando)\n",
    "        probs = 1 / (1 + gamma * torch.clip(fhatstar - fhatrando, min=0))\n",
    "        unif = torch.rand(size=(N, 1), device=X.device)\n",
    "        shouldexplore = (unif <= probs).long()\n",
    "        return (ahatstar + shouldexplore * (rando - ahatstar)), algo, invpalgo, shouldexplore, ahatstar\n",
    "\n",
    "def bestconstant(dataset, *, cost, price):\n",
    "    import numpy\n",
    "        \n",
    "    constreward = [EasyAcc() for _ in range(100)]\n",
    "    punder = [EasyAcc() for _ in range(100)]\n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "    \n",
    "    for _, Ys in generator:\n",
    "        with torch.no_grad():\n",
    "            for n in range(len(constreward)):\n",
    "                z = n / len(constreward)\n",
    "                constreward[n] += torch.mean(profit(z, Ys, cost=cost, price=price)).item()\n",
    "                punder[n] += torch.mean((z < Ys).float()).item()\n",
    "    \n",
    "    return { 'best_constant_reward': max(( (v.mean(), punder[n].mean(), n/len(constreward)) for n, v in enumerate(constreward) ))\n",
    "           }\n",
    "\n",
    "def bootstrap(data, stat, conf):\n",
    "    from math import ceil\n",
    "    import numpy as np\n",
    "    \n",
    "    samples = ceil(3 / conf)\n",
    "    stats = []\n",
    "    for _ in range(samples):\n",
    "        stats.append(stat(np.random.choice(data, len(data))))\n",
    "        \n",
    "    return np.quantile(stats, q=[conf, 1-conf])\n",
    "            \n",
    "def compute_evar_ci(rewards, q):\n",
    "    import numpy as np\n",
    "    \n",
    "    def f(m, data):\n",
    "        return q * np.sum(np.square(np.clip(data - m, a_min=0, a_max=None))) + (1 - q) * np.sum(np.square(np.clip(m - data, a_min=0, a_max=None)))  \n",
    "            \n",
    "    def evar(data):\n",
    "        from scipy.optimize import minimize_scalar\n",
    "        res = minimize_scalar(lambda m: f(m, data), bounds=(np.min(data), np.max(data)), method='bounded')\n",
    "        assert res.success\n",
    "        return res.x\n",
    "    \n",
    "    return bootstrap(rewards, evar, conf=0.05)\n",
    "        \n",
    "def learnOnline(dataset, *, q, seed, batch_size, modelfactory, initlr, tzero, eta, gammamin, gammamax, nalgos, cost, price):\n",
    "    import numpy as np\n",
    "    import time\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "        \n",
    "    generator = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    model = None\n",
    "    l1_loss = torch.nn.L1Loss(reduction='none')\n",
    "    squared_loss = torch.nn.MSELoss(reduction='none')\n",
    "    \n",
    "    print('{:<5s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<10s}'.format(\n",
    "            'n', 'loss', 'since last', 'rahat', 'since last', f'evarq({q})', 'since last', 'punder', 'since last', 'accept', 'since last', 'dt (sec)'),\n",
    "          flush=True)\n",
    "    avloss, sincelast, acc, accsincelast, avreward, rewardsincelast = [ \n",
    "        EasyPoissonBootstrapAcc(batch_size=batch_size) for _ in range(6) ]\n",
    "    accept, acceptsincelast, punder, pundersincelast = [\n",
    "        EasyPoissonBootstrapAcc(batch_size=batch_size) for _ in range(4) ]\n",
    "    allrewards, allrewardssincelast, punders = [], [], []\n",
    "    \n",
    "    for bno, (Xs, ys) in enumerate(generator):\n",
    "        if model is None:\n",
    "            from math import sqrt\n",
    "            model = modelfactory(Xs)\n",
    "            opt = torch.optim.Adam(( p for p in model.parameters() if p.requires_grad ), lr=initlr)\n",
    "            scheduler = torch.optim.lr_scheduler.LambdaLR(opt, lr_lambda = lambda t: sqrt(tzero) / sqrt(tzero + t))\n",
    "            sampler = CorralIGW(eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, device=Xs.device)\n",
    "            start = time.time()\n",
    "            \n",
    "        opt.zero_grad()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            sample, algo, invpalgo, shouldexplore, ahatstar = sampler.sample(model, Xs)\n",
    "            reward = profit(sample, ys, cost=cost, price=price)\n",
    "            allrewards.append(reward.numpy())\n",
    "            allrewardssincelast.append(reward.numpy())\n",
    "            rahat = profit(ahatstar, ys, cost=cost, price=price)\n",
    "        \n",
    "        score = model(Xs, sample)\n",
    "        with torch.no_grad():\n",
    "            factor = q * (score < reward).long() + (1 - q) * (score >= reward).long()\n",
    "        loss = 2 * torch.mean(factor * squared_loss(score, reward))\n",
    "        loss.backward()\n",
    "        opt.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            acc += torch.mean(rahat).item()\n",
    "            accsincelast += torch.mean(rahat).item()\n",
    "            avreward += torch.mean(reward).item()\n",
    "            rewardsincelast += torch.mean(reward).item()\n",
    "            punder += torch.mean((sample < ys).float()).item()\n",
    "            pundersincelast += torch.mean((sample < ys).float()).item()\n",
    "            avloss += loss.item()\n",
    "            sincelast += loss.item()\n",
    "            accept += torch.mean(shouldexplore.float()).item()\n",
    "            acceptsincelast += torch.mean(shouldexplore.float()).item()\n",
    "            sampler.update(algo, invpalgo, reward)\n",
    "\n",
    "        if bno & (bno - 1) == 0:\n",
    "            evar = compute_evar_ci(np.concatenate(allrewards, axis=None), q)\n",
    "            evarsincelast = compute_evar_ci(np.concatenate(allrewardssincelast, axis=None), q)\n",
    "            punders.append((bno, punder.ci()))\n",
    "            \n",
    "            print('{:<5d}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<10.5f}'.format(\n",
    "                    avloss.n, avloss.formatci(), sincelast.formatci(), acc.formatci(),\n",
    "                    accsincelast.formatci(), f'[{evar[0]:.4f},{evar[1]:.4f}]', f'[{evarsincelast[0]:.4f},{evarsincelast[1]:.4f}]',\n",
    "                    punder.formatci(), pundersincelast.formatci(),\n",
    "                    accept.formatci(), acceptsincelast.formatci(),\n",
    "                    time.time() - start),\n",
    "                  flush=True)\n",
    "            sincelast, accsincelast, rewardsincelast, acceptsincelast, pundersincelast = [ \n",
    "                EasyPoissonBootstrapAcc(batch_size=batch_size) for _ in range(5) ]\n",
    "            allrewardssincelast = []\n",
    "            print(f'sampler.palgo = { 1/sampler.invpalgo }')\n",
    "\n",
    "    evar = compute_evar_ci(np.concatenate(allrewards, axis=None), q)\n",
    "    evarsincelast = compute_evar_ci(np.concatenate(allrewardssincelast, axis=None), q)\n",
    "    punders.append((bno, punder.ci()))\n",
    "    print('{:<5d}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<17s}\\t{:<10.5f}'.format(\n",
    "            avloss.n, avloss.formatci(), sincelast.formatci(), acc.formatci(),\n",
    "            accsincelast.formatci(), f'[{evar[0]:.4f},{evar[1]:.4f}]', f'[{evarsincelast[0]:.4f},{evarsincelast[1]:.4f}]',\n",
    "            punder.formatci(), pundersincelast.formatci(),\n",
    "            accept.formatci(), acceptsincelast.formatci(),\n",
    "            time.time() - start),\n",
    "          flush=True)\n",
    "    print(f'sampler.palgo = { 1/sampler.invpalgo }')\n",
    "    print(f'0.2: {compute_evar_ci(np.concatenate(allrewards, axis=None), 0.2)} 0.5: {compute_evar_ci(np.concatenate(allrewards, axis=None), 0.5)}')\n",
    "    return punders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f07046",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Chicago"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "723b855d",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_reward': (0.02581233765993202, 0.32454251386431804, 0.13)}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = ArffToPytorch('chicago.arff', target='count')\n",
    "bestconstant(mydata, cost=1/3, price=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58597d4a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### risk-neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "ad296db6",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.5)       \tsince last       \tpunder           \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.0508,0.2033]  \t[0.0508,0.2033]  \t[-0.2333,-0.0583]\t[-0.2333,-0.0583]\t[-0.1912,-0.0734]\t[-0.1865,-0.0993]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.1406,0.5625]  \t[0.1406,0.5625]  \t0.15077   \n",
      "sampler.palgo = tensor([0.2494, 0.2492, 0.2478, 0.2536])\n",
      "2    \t[0.0767,0.1959]  \t[0.0514,0.2057]  \t[-0.2271,-0.0891]\t[-0.2411,-0.0603]\t[-0.1839,-0.1085]\t[-0.1901,-0.1199]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.1719,0.4535]  \t[0.0938,0.3750]  \t0.27002   \n",
      "sampler.palgo = tensor([0.2583, 0.2519, 0.2511, 0.2387])\n",
      "3    \t[0.0817,0.1681]  \t[0.0395,0.1578]  \t[-0.2046,-0.0998]\t[-0.2090,-0.0523]\t[-0.1705,-0.1190]\t[-0.1541,-0.0933]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.2812,0.5690]  \t[0.2344,0.9375]  \t0.39572   \n",
      "sampler.palgo = tensor([0.2604, 0.2552, 0.2487, 0.2357])\n",
      "5    \t[0.0710,0.1217]  \t[0.0243,0.0656]  \t[-0.1309,-0.0688]\t[-0.0297,-0.0036]\t[-0.1161,-0.0625]\t[-0.0635,0.0205] \t[0.0123,0.0375]  \t[0.0312,0.0941]  \t[0.2559,0.4689]  \t[0.1402,0.3598]  \t0.54224   \n",
      "sampler.palgo = tensor([0.2648, 0.2643, 0.2418, 0.2291])\n",
      "9    \t[0.0434,0.0733]  \t[0.0084,0.0183]  \t[-0.0631,-0.0287]\t[0.0152,0.0299]  \t[-0.0719,-0.0195]\t[-0.0179,0.0425] \t[0.0815,0.1374]  \t[0.1443,0.2734]  \t[0.2222,0.3838]  \t[0.1596,0.3400]  \t0.71110   \n",
      "sampler.palgo = tensor([0.2485, 0.2764, 0.2419, 0.2332])\n",
      "17   \t[0.0238,0.0396]  \t[0.0012,0.0022]  \t[-0.0264,-0.0071]\t[0.0112,0.0190]  \t[-0.0409,-0.0139]\t[-0.0185,0.0105] \t[0.2021,0.3173]  \t[0.3222,0.5244]  \t[0.1939,0.2958]  \t[0.1287,0.2092]  \t0.92954   \n",
      "sampler.palgo = tensor([0.2292, 0.2800, 0.2498, 0.2409])\n",
      "33   \t[0.0135,0.0222]  \t[0.0028,0.0045]  \t[-0.0082,0.0016] \t[0.0099,0.0134]  \t[-0.0210,-0.0021]\t[-0.0089,0.0053] \t[0.3238,0.4105]  \t[0.4043,0.5538]  \t[0.2096,0.2704]  \t[0.2049,0.2852]  \t1.28400   \n",
      "sampler.palgo = tensor([0.2137, 0.2799, 0.2619, 0.2445])\n",
      "65   \t[0.0090,0.0138]  \t[0.0036,0.0068]  \t[0.0020,0.0071]  \t[0.0108,0.0137]  \t[-0.0074,0.0014] \t[-0.0003,0.0134] \t[0.4055,0.4767]  \t[0.4653,0.5929]  \t[0.2038,0.2416]  \t[0.1757,0.2267]  \t1.60298   \n",
      "sampler.palgo = tensor([0.1742, 0.3116, 0.2645, 0.2498])\n",
      "129  \t[0.0060,0.0086]  \t[0.0026,0.0037]  \t[0.0121,0.0154]  \t[0.0214,0.0252]  \t[-0.0004,0.0058] \t[0.0042,0.0139]  \t[0.4117,0.4496]  \t[0.3812,0.4534]  \t[0.2141,0.2427]  \t[0.2167,0.2540]  \t2.03593   \n",
      "sampler.palgo = tensor([0.1174, 0.3193, 0.2736, 0.2898])\n",
      "257  \t[0.0046,0.0059]  \t[0.0029,0.0033]  \t[0.0241,0.0265]  \t[0.0344,0.0386]  \t[0.0130,0.0174]  \t[0.0248,0.0299]  \t[0.3206,0.3443]  \t[0.2175,0.2427]  \t[0.2192,0.2396]  \t[0.2140,0.2397]  \t2.78291   \n",
      "sampler.palgo = tensor([0.0773, 0.2192, 0.3340, 0.3695])\n",
      "513  \t[0.0039,0.0045]  \t[0.0031,0.0033]  \t[0.0326,0.0347]  \t[0.0402,0.0436]  \t[0.0225,0.0265]  \t[0.0305,0.0372]  \t[0.2599,0.2748]  \t[0.1918,0.2079]  \t[0.2003,0.2115]  \t[0.1760,0.1916]  \t4.11898   \n",
      "sampler.palgo = tensor([0.0387, 0.1551, 0.3552, 0.4511])\n",
      "1025 \t[0.0035,0.0038]  \t[0.0029,0.0031]  \t[0.0373,0.0388]  \t[0.0410,0.0435]  \t[0.0292,0.0321]  \t[0.0349,0.0390]  \t[0.2277,0.2360]  \t[0.1915,0.2033]  \t[0.1807,0.1884]  \t[0.1589,0.1702]  \t6.62962   \n",
      "sampler.palgo = tensor([0.0235, 0.0871, 0.5539, 0.3355])\n",
      "2049 \t[0.0031,0.0033]  \t[0.0027,0.0028]  \t[0.0394,0.0406]  \t[0.0414,0.0430]  \t[0.0332,0.0352]  \t[0.0371,0.0394]  \t[0.2102,0.2158]  \t[0.1913,0.1981]  \t[0.1620,0.1672]  \t[0.1406,0.1472]  \t11.51172  \n",
      "sampler.palgo = tensor([0.0145, 0.0503, 0.3641, 0.5711])\n",
      "4097 \t[0.0029,0.0030]  \t[0.0026,0.0027]  \t[0.0413,0.0422]  \t[0.0428,0.0438]  \t[0.0371,0.0382]  \t[0.0400,0.0419]  \t[0.2007,0.2046]  \t[0.1888,0.1949]  \t[0.1325,0.1350]  \t[0.1015,0.1049]  \t22.60583  \n",
      "sampler.palgo = tensor([0.0063, 0.0205, 0.2032, 0.7701])\n",
      "4328 \t[0.0029,0.0030]  \t[0.0025,0.0027]  \t[0.0414,0.0422]  \t[0.0420,0.0456]  \t[0.0373,0.0382]  \t[0.0389,0.0439]  \t[0.2001,0.2039]  \t[0.1825,0.1975]  \t[0.1299,0.1326]  \t[0.0818,0.0900]  \t24.12216  \n",
      "sampler.palgo = tensor([0.0058, 0.0181, 0.1972, 0.7789])\n",
      "0.2: [0.00701554 0.00811809] 0.5: [0.03735941 0.03833336]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-biking-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.039249862458051156, 8.149927760620125, 0.4594617772043446, 8, 1024, 4\n",
    "    cost, price = 1/3, 1\n",
    "    \n",
    "    learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, cost=cost, price=price,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', cost=cost, price=price, approxargmax=True))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d22ab25",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### risk-averse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "942a45f1",
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.2)       \tsince last       \tpunder           \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.0816,0.3266]  \t[0.0816,0.3266]  \t[-0.2335,-0.0584]\t[-0.2335,-0.0584]\t[-0.1938,-0.1548]\t[-0.1929,-0.1468]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.1875,0.7500]  \t[0.1875,0.7500]  \t0.13716   \n",
      "sampler.palgo = tensor([0.1640, 0.1643, 0.1707, 0.1630, 0.1707, 0.1672])\n",
      "2    \t[0.1177,0.3026]  \t[0.0760,0.3039]  \t[-0.2275,-0.0892]\t[-0.2415,-0.0604]\t[-0.1928,-0.1495]\t[-0.1914,-0.1616]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.1637,0.4535]  \t[0.0469,0.1875]  \t0.26740   \n",
      "sampler.palgo = tensor([0.1682, 0.1685, 0.1729, 0.1592, 0.1678, 0.1635])\n",
      "3    \t[0.1337,0.2767]  \t[0.0714,0.2855]  \t[-0.2055,-0.1004]\t[-0.2122,-0.0531]\t[-0.1835,-0.1548]\t[-0.1787,-0.1461]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.1872,0.3971]  \t[0.0938,0.3750]  \t0.39766   \n",
      "sampler.palgo = tensor([0.1697, 0.1664, 0.1691, 0.1630, 0.1644, 0.1675])\n",
      "5    \t[0.1143,0.1997]  \t[0.0399,0.1064]  \t[-0.1358,-0.0768]\t[-0.0507,-0.0163]\t[-0.1613,-0.1261]\t[-0.1162,-0.0305]\t[0.0123,0.0406]  \t[0.0234,0.0938]  \t[0.2280,0.4062]  \t[0.2180,0.5312]  \t0.54528   \n",
      "sampler.palgo = tensor([0.1735, 0.1602, 0.1692, 0.1588, 0.1725, 0.1658])\n",
      "9    \t[0.0707,0.1201]  \t[0.0162,0.0303]  \t[-0.0745,-0.0416]\t[-0.0015,0.0081] \t[-0.1259,-0.0920]\t[-0.0729,-0.0213]\t[0.0381,0.0713]  \t[0.0508,0.1217]  \t[0.2220,0.3530]  \t[0.1953,0.3752]  \t0.68685   \n",
      "sampler.palgo = tensor([0.1691, 0.1658, 0.1699, 0.1601, 0.1732, 0.1619])\n",
      "17   \t[0.0387,0.0655]  \t[0.0022,0.0036]  \t[-0.0332,-0.0142]\t[0.0107,0.0189]  \t[-0.1067,-0.0706]\t[-0.0891,-0.0244]\t[0.1745,0.2759]  \t[0.3065,0.5124]  \t[0.2395,0.3376]  \t[0.2064,0.3202]  \t0.86793   \n",
      "sampler.palgo = tensor([0.1658, 0.1608, 0.1717, 0.1666, 0.1671, 0.1679])\n",
      "33   \t[0.0203,0.0341]  \t[0.0006,0.0009]  \t[-0.0115,-0.0020]\t[0.0095,0.0128]  \t[-0.0788,-0.0534]\t[-0.0352,-0.0056]\t[0.3521,0.4508]  \t[0.4843,0.6730]  \t[0.1908,0.2529]  \t[0.1269,0.1896]  \t1.11851   \n",
      "sampler.palgo = tensor([0.1553, 0.1604, 0.1749, 0.1700, 0.1699, 0.1694])\n",
      "65   \t[0.0105,0.0176]  \t[0.0003,0.0006]  \t[-0.0013,0.0035] \t[0.0080,0.0102]  \t[-0.0483,-0.0343]\t[-0.0181,-0.0037]\t[0.4603,0.5308]  \t[0.5342,0.6773]  \t[0.1598,0.1996]  \t[0.1185,0.1613]  \t1.46702   \n",
      "sampler.palgo = tensor([0.1404, 0.1570, 0.1812, 0.1730, 0.1711, 0.1773])\n",
      "129  \t[0.0060,0.0095]  \t[0.0013,0.0018]  \t[0.0041,0.0067]  \t[0.0089,0.0104]  \t[-0.0353,-0.0247]\t[-0.0269,-0.0103]\t[0.5369,0.5869]  \t[0.5717,0.6685]  \t[0.1491,0.1706]  \t[0.1240,0.1522]  \t2.01275   \n",
      "sampler.palgo = tensor([0.1265, 0.1486, 0.1680, 0.1898, 0.1755, 0.1917])\n",
      "257  \t[0.0035,0.0052]  \t[0.0008,0.0010]  \t[0.0070,0.0084]  \t[0.0095,0.0105]  \t[-0.0272,-0.0184]\t[-0.0180,-0.0068]\t[0.5642,0.6042]  \t[0.5648,0.6248]  \t[0.1530,0.1667]  \t[0.1490,0.1670]  \t2.84807   \n",
      "sampler.palgo = tensor([0.0974, 0.1308, 0.1625, 0.2061, 0.1967, 0.2065])\n",
      "513  \t[0.0020,0.0029]  \t[0.0005,0.0006]  \t[0.0088,0.0096]  \t[0.0103,0.0110]  \t[-0.0206,-0.0154]\t[-0.0163,-0.0085]\t[0.5944,0.6219]  \t[0.6152,0.6629]  \t[0.1404,0.1489]  \t[0.1245,0.1349]  \t4.75451   \n",
      "sampler.palgo = tensor([0.0589, 0.0973, 0.1415, 0.2328, 0.2237, 0.2459])\n",
      "1025 \t[0.0013,0.0017]  \t[0.0005,0.0006]  \t[0.0099,0.0105]  \t[0.0109,0.0114]  \t[-0.0141,-0.0104]\t[-0.0081,-0.0041]\t[0.6157,0.6347]  \t[0.6244,0.6556]  \t[0.1243,0.1296]  \t[0.1059,0.1144]  \t7.56502   \n",
      "sampler.palgo = tensor([0.0292, 0.0692, 0.1119, 0.2671, 0.2285, 0.2941])\n",
      "2049 \t[0.0008,0.0011]  \t[0.0004,0.0004]  \t[0.0117,0.0120]  \t[0.0133,0.0138]  \t[-0.0069,-0.0048]\t[-0.0014,0.0005] \t[0.6124,0.6259]  \t[0.6026,0.6220]  \t[0.1035,0.1070]  \t[0.0819,0.0867]  \t13.36482  \n",
      "sampler.palgo = tensor([0.0174, 0.0375, 0.0827, 0.1933, 0.2693, 0.3999])\n",
      "4097 \t[0.0008,0.0009]  \t[0.0008,0.0008]  \t[0.0234,0.0238]  \t[0.0349,0.0358]  \t[0.0029,0.0040]  \t[0.0103,0.0114]  \t[0.5098,0.5175]  \t[0.4017,0.4133]  \t[0.0931,0.0956]  \t[0.0818,0.0850]  \t24.98399  \n",
      "sampler.palgo = tensor([0.0075, 0.0129, 0.0273, 0.1227, 0.1339, 0.6957])\n",
      "4328 \t[0.0008,0.0009]  \t[0.0009,0.0009]  \t[0.0243,0.0249]  \t[0.0413,0.0445]  \t[0.0037,0.0045]  \t[0.0129,0.0164]  \t[0.5002,0.5077]  \t[0.3201,0.3428]  \t[0.0918,0.0943]  \t[0.0687,0.0762]  \t26.52638  \n",
      "sampler.palgo = tensor([0.0067, 0.0123, 0.0230, 0.1326, 0.1448, 0.6806])\n",
      "0.2: [0.00364617 0.00468824] 0.5: [0.02178249 0.02254296]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-biking-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.049873098247357875, 1.7785963392968864, 0.7748845548802411, 8, 2048, 6\n",
    "    cost, price = 1/3, 1\n",
    "    \n",
    "    learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, cost=cost, price=price,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', cost=cost, price=price, approxargmax=True))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61de5c5f",
   "metadata": {},
   "source": [
    "## London"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e8e233e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_reward': (0.04380489668783245, 0.33071483182250905, 0.17)}"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = ArffToPytorch('london.arff', target='count')\n",
    "bestconstant(mydata, cost=1/3, price=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7066a550",
   "metadata": {},
   "source": [
    "### risk-neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "d57dbee9",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.5)       \tsince last       \tpunder           \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.0407,0.1628]  \t[0.0407,0.1628]  \t[-0.1450,-0.0363]\t[-0.1450,-0.0363]\t[-0.1521,-0.0543]\t[-0.1459,-0.0658]\t[0.0469,0.1875]  \t[0.0469,0.1875]  \t[0.0938,0.3750]  \t[0.0938,0.3750]  \t0.14097   \n",
      "sampler.palgo = tensor([0.2546, 0.2512, 0.2415, 0.2527])\n",
      "2    \t[0.0562,0.1455]  \t[0.0349,0.1397]  \t[-0.1375,-0.0537]\t[-0.1419,-0.0355]\t[-0.1258,-0.0305]\t[-0.1513,0.0176] \t[0.0234,0.0938]  \t[0.0000,0.0000]  \t[0.1719,0.4457]  \t[0.1406,0.5625]  \t0.26526   \n",
      "sampler.palgo = tensor([0.2625, 0.2530, 0.2340, 0.2505])\n",
      "3    \t[0.0545,0.1151]  \t[0.0219,0.0875]  \t[-0.0792,-0.0181]\t[0.0160,0.0640]  \t[-0.0860,0.0040] \t[-0.0799,0.1148] \t[0.0156,0.0625]  \t[0.0000,0.0000]  \t[0.1820,0.3812]  \t[0.0938,0.3750]  \t0.41818   \n",
      "sampler.palgo = tensor([0.2771, 0.2595, 0.2199, 0.2435])\n",
      "5    \t[0.0698,0.1201]  \t[0.0629,0.1649]  \t[-0.1011,-0.0520]\t[-0.1850,-0.0699]\t[-0.1171,-0.0552]\t[-0.1604,-0.0941]\t[0.0094,0.0375]  \t[0.0000,0.0000]  \t[0.2275,0.4250]  \t[0.2027,0.5551]  \t0.57015   \n",
      "sampler.palgo = tensor([0.2526, 0.2539, 0.2244, 0.2691])\n",
      "9    \t[0.0583,0.0860]  \t[0.0334,0.0578]  \t[-0.0641,-0.0343]\t[-0.0234,-0.0005]\t[-0.0757,-0.0304]\t[-0.0485,0.0124] \t[0.0416,0.0885]  \t[0.0859,0.1723]  \t[0.2727,0.4168]  \t[0.2885,0.4852]  \t0.71321   \n",
      "sampler.palgo = tensor([0.2457, 0.2278, 0.2157, 0.3108])\n",
      "17   \t[0.0364,0.0517]  \t[0.0095,0.0157]  \t[-0.0192,-0.0006]\t[0.0207,0.0404]  \t[-0.0356,0.0037] \t[0.0122,0.0449]  \t[0.0983,0.1491]  \t[0.1367,0.2391]  \t[0.2205,0.3209]  \t[0.1523,0.2287]  \t0.88757   \n",
      "sampler.palgo = tensor([0.2417, 0.2303, 0.2321, 0.2959])\n",
      "33   \t[0.0200,0.0279]  \t[0.0023,0.0031]  \t[0.0085,0.0187]  \t[0.0337,0.0463]  \t[-0.0032,0.0218] \t[0.0243,0.0424]  \t[0.2460,0.3221]  \t[0.3934,0.5421]  \t[0.2007,0.2554]  \t[0.1551,0.2199]  \t1.12178   \n",
      "sampler.palgo = tensor([0.2286, 0.2218, 0.2422, 0.3074])\n",
      "65   \t[0.0112,0.0154]  \t[0.0018,0.0026]  \t[0.0226,0.0289]  \t[0.0341,0.0438]  \t[0.0158,0.0265]  \t[0.0291,0.0394]  \t[0.3622,0.4168]  \t[0.4387,0.5651]  \t[0.1865,0.2210]  \t[0.1572,0.2066]  \t1.41195   \n",
      "sampler.palgo = tensor([0.1642, 0.2335, 0.2503, 0.3519])\n",
      "129  \t[0.0071,0.0094]  \t[0.0028,0.0035]  \t[0.0317,0.0361]  \t[0.0390,0.0448]  \t[0.0270,0.0329]  \t[0.0345,0.0430]  \t[0.3803,0.4242]  \t[0.3801,0.4513]  \t[0.1627,0.1847]  \t[0.1313,0.1568]  \t1.87075   \n",
      "sampler.palgo = tensor([0.1126, 0.2778, 0.2892, 0.3203])\n",
      "257  \t[0.0057,0.0070]  \t[0.0042,0.0046]  \t[0.0413,0.0439]  \t[0.0484,0.0536]  \t[0.0358,0.0404]  \t[0.0433,0.0494]  \t[0.3355,0.3595]  \t[0.2700,0.3027]  \t[0.1604,0.1737]  \t[0.1492,0.1674]  \t2.67933   \n",
      "sampler.palgo = tensor([0.0638, 0.1856, 0.3975, 0.3530])\n",
      "513  \t[0.0050,0.0056]  \t[0.0041,0.0044]  \t[0.0455,0.0481]  \t[0.0493,0.0531]  \t[0.0413,0.0449]  \t[0.0447,0.0497]  \t[0.3055,0.3221]  \t[0.2737,0.2926]  \t[0.1432,0.1515]  \t[0.1198,0.1331]  \t4.11681   \n",
      "sampler.palgo = tensor([0.0305, 0.1262, 0.5423, 0.3010])\n",
      "1025 \t[0.0044,0.0047]  \t[0.0037,0.0039]  \t[0.0486,0.0505]  \t[0.0509,0.0537]  \t[0.0448,0.0476]  \t[0.0482,0.0518]  \t[0.2847,0.2955]  \t[0.2598,0.2737]  \t[0.1262,0.1318]  \t[0.1060,0.1128]  \t6.76028   \n",
      "sampler.palgo = tensor([0.0153, 0.0476, 0.6533, 0.2838])\n",
      "2049 \t[0.0040,0.0041]  \t[0.0035,0.0036]  \t[0.0499,0.0513]  \t[0.0511,0.0526]  \t[0.0471,0.0489]  \t[0.0491,0.0515]  \t[0.2819,0.2915]  \t[0.2794,0.2891]  \t[0.1095,0.1130]  \t[0.0904,0.0953]  \t12.20412  \n",
      "sampler.palgo = tensor([0.0081, 0.0478, 0.8155, 0.1287])\n",
      "2177 \t[0.0040,0.0041]  \t[0.0033,0.0037]  \t[0.0500,0.0514]  \t[0.0496,0.0548]  \t[0.0475,0.0495]  \t[0.0490,0.0541]  \t[0.2815,0.2908]  \t[0.2645,0.2954]  \t[0.1077,0.1109]  \t[0.0758,0.0873]  \t13.17079  \n",
      "sampler.palgo = tensor([0.0071, 0.0366, 0.8146, 0.1417])\n",
      "0.2: [0.00919939 0.01115235] 0.5: [0.04740603 0.04901608]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-biking-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.015162208381891886, 6.286877164763437, 0.9636644839634085, 16, 4096, 4\n",
    "    cost, price = 1/3, 1\n",
    "    \n",
    "    learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, cost=cost, price=price,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', cost=cost, price=price, approxargmax=True))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9227a6ae",
   "metadata": {},
   "source": [
    "### risk-averse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "0e2b206a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.2)       \tsince last       \tpunder           \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.0651,0.2605]  \t[0.0651,0.2605]  \t[-0.1450,-0.0363]\t[-0.1450,-0.0363]\t[-0.1656,-0.0921]\t[-0.1719,-0.1117]\t[0.0469,0.1875]  \t[0.0469,0.1875]  \t[0.0938,0.3750]  \t[0.0938,0.3750]  \t0.14577   \n",
      "sampler.palgo = tensor([0.2520, 0.2505, 0.2463, 0.2512])\n",
      "2    \t[0.0884,0.2293]  \t[0.0539,0.2158]  \t[-0.1367,-0.0534]\t[-0.1402,-0.0350]\t[-0.1628,-0.1033]\t[-0.1749,-0.0908]\t[0.0234,0.0938]  \t[0.0000,0.0000]  \t[0.1719,0.4457]  \t[0.1406,0.5625]  \t0.27650   \n",
      "sampler.palgo = tensor([0.2605, 0.2464, 0.2430, 0.2502])\n",
      "3    \t[0.0840,0.1794]  \t[0.0321,0.1284]  \t[-0.0774,-0.0163]\t[0.0175,0.0700]  \t[-0.1391,-0.0972]\t[-0.1408,0.0634] \t[0.0156,0.0625]  \t[0.0000,0.0000]  \t[0.1820,0.3812]  \t[0.0938,0.3750]  \t0.42670   \n",
      "sampler.palgo = tensor([0.2667, 0.2495, 0.2365, 0.2473])\n",
      "5    \t[0.1005,0.1694]  \t[0.0766,0.1996]  \t[-0.0938,-0.0464]\t[-0.1632,-0.0610]\t[-0.1555,-0.1134]\t[-0.1963,-0.1245]\t[0.0094,0.0375]  \t[0.0000,0.0000]  \t[0.2497,0.4323]  \t[0.2496,0.6254]  \t0.58760   \n",
      "sampler.palgo = tensor([0.2591, 0.2452, 0.2397, 0.2560])\n",
      "9    \t[0.0671,0.1068]  \t[0.0194,0.0387]  \t[-0.0409,-0.0137]\t[0.0161,0.0373]  \t[-0.1261,-0.0822]\t[-0.0662,-0.0055]\t[0.0588,0.1235]  \t[0.1246,0.2660]  \t[0.2691,0.4013]  \t[0.2344,0.4383]  \t0.78476   \n",
      "sampler.palgo = tensor([0.2568, 0.2337, 0.2428, 0.2667])\n",
      "17   \t[0.0367,0.0578]  \t[0.0020,0.0033]  \t[-0.0073,0.0114] \t[0.0254,0.0408]  \t[-0.0851,-0.0426]\t[-0.0370,0.0082] \t[0.2113,0.3248]  \t[0.3417,0.5510]  \t[0.1999,0.2803]  \t[0.1131,0.1720]  \t0.99757   \n",
      "sampler.palgo = tensor([0.2518, 0.2391, 0.2451, 0.2640])\n",
      "33   \t[0.0192,0.0302]  \t[0.0007,0.0010]  \t[0.0077,0.0167]  \t[0.0193,0.0261]  \t[-0.0512,-0.0267]\t[-0.0115,0.0124] \t[0.4084,0.5119]  \t[0.5757,0.7814]  \t[0.1745,0.2259]  \t[0.1298,0.1809]  \t1.24957   \n",
      "sampler.palgo = tensor([0.2492, 0.2339, 0.2514, 0.2656])\n",
      "65   \t[0.0103,0.0157]  \t[0.0007,0.0012]  \t[0.0128,0.0180]  \t[0.0169,0.0215]  \t[-0.0323,-0.0158]\t[-0.0067,0.0077] \t[0.5236,0.6085]  \t[0.5898,0.7595]  \t[0.1567,0.1864]  \t[0.1250,0.1632]  \t1.66519   \n",
      "sampler.palgo = tensor([0.2326, 0.2383, 0.2541, 0.2750])\n",
      "129  \t[0.0057,0.0084]  \t[0.0007,0.0010]  \t[0.0156,0.0184]  \t[0.0172,0.0199]  \t[-0.0170,-0.0029]\t[-0.0024,0.0089] \t[0.5969,0.6571]  \t[0.6351,0.7279]  \t[0.1574,0.1754]  \t[0.1469,0.1734]  \t2.29144   \n",
      "sampler.palgo = tensor([0.2221, 0.2448, 0.2702, 0.2628])\n",
      "257  \t[0.0032,0.0046]  \t[0.0007,0.0009]  \t[0.0186,0.0200]  \t[0.0202,0.0225]  \t[-0.0103,-0.0023]\t[-0.0090,0.0022] \t[0.6314,0.6770]  \t[0.6392,0.7086]  \t[0.1427,0.1547]  \t[0.1186,0.1364]  \t3.32104   \n",
      "sampler.palgo = tensor([0.1917, 0.2278, 0.2753, 0.3052])\n",
      "513  \t[0.0020,0.0026]  \t[0.0006,0.0008]  \t[0.0211,0.0223]  \t[0.0234,0.0250]  \t[-0.0028,0.0014] \t[0.0011,0.0063]  \t[0.6321,0.6650]  \t[0.6267,0.6688]  \t[0.1341,0.1425]  \t[0.1220,0.1331]  \t5.93293   \n",
      "sampler.palgo = tensor([0.1403, 0.1971, 0.2929, 0.3696])\n",
      "1025 \t[0.0014,0.0018]  \t[0.0009,0.0010]  \t[0.0274,0.0284]  \t[0.0334,0.0350]  \t[0.0034,0.0060]  \t[0.0079,0.0108]  \t[0.6216,0.6417]  \t[0.6013,0.6290]  \t[0.1207,0.1251]  \t[0.1037,0.1125]  \t8.97438   \n",
      "sampler.palgo = tensor([0.0635, 0.1799, 0.3321, 0.4246])\n",
      "2049 \t[0.0013,0.0015]  \t[0.0012,0.0012]  \t[0.0369,0.0378]  \t[0.0462,0.0475]  \t[0.0110,0.0125]  \t[0.0175,0.0193]  \t[0.5691,0.5833]  \t[0.5138,0.5283]  \t[0.1069,0.1095]  \t[0.0911,0.0951]  \t14.54108  \n",
      "sampler.palgo = tensor([0.0313, 0.1633, 0.2678, 0.5375])\n",
      "2177 \t[0.0013,0.0015]  \t[0.0011,0.0013]  \t[0.0376,0.0386]  \t[0.0469,0.0517]  \t[0.0115,0.0132]  \t[0.0204,0.0259]  \t[0.5635,0.5785]  \t[0.4669,0.5138]  \t[0.1048,0.1073]  \t[0.0668,0.0769]  \t15.54726  \n",
      "sampler.palgo = tensor([0.0304, 0.1583, 0.2735, 0.5377])\n",
      "0.2: [0.01153899 0.01289424] 0.5: [0.0352163  0.03633312]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-biking-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.02892759212511789, 5.014926693720849, 0.41488348196455327, 16, 4096, 4 \n",
    "    cost, price = 1/3, 1\n",
    "\n",
    "    learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, cost=cost, price=price,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', cost=cost, price=price, approxargmax=True))\n",
    "doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b2273ac",
   "metadata": {},
   "source": [
    "## DC 2012"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20988a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_constant_reward': (0.05713017879101886, 0.33249080882352944, 0.23)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydata = ArffToPytorch('dc2012.arff', target='count')\n",
    "bestconstant(mydata, cost=1/3, price=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66960a7d",
   "metadata": {},
   "source": [
    "### risk-neutral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5dabf7d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.5)       \tsince last       \tpunder           \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.0470,0.1879]  \t[0.0470,0.1879]  \t[-0.1104,-0.0276]\t[-0.1104,-0.0276]\t[-0.1795,0.0192] \t[-0.1802,0.0251] \t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.0469,0.1875]  \t[0.0469,0.1875]  \t0.15675   \n",
      "sampler.palgo = tensor([0.2578, 0.2394, 0.2375, 0.2654])\n",
      "2    \t[0.0529,0.1414]  \t[0.0259,0.1036]  \t[-0.0623,-0.0218]\t[-0.0169,-0.0042]\t[-0.1049,0.0098] \t[-0.1021,0.0941] \t[0.0312,0.0941]  \t[0.0469,0.1875]  \t[0.1090,0.2656]  \t[0.0938,0.3750]  \t0.29408   \n",
      "sampler.palgo = tensor([0.2591, 0.2402, 0.2440, 0.2567])\n",
      "3    \t[0.0666,0.1401]  \t[0.0401,0.1602]  \t[-0.0799,-0.0402]\t[-0.1362,-0.0340]\t[-0.1025,-0.0150]\t[-0.1728,-0.0008]\t[0.0570,0.1201]  \t[0.0469,0.1875]  \t[0.1661,0.3596]  \t[0.1406,0.5625]  \t0.43858   \n",
      "sampler.palgo = tensor([0.2816, 0.2276, 0.2269, 0.2638])\n",
      "5    \t[0.0650,0.1129]  \t[0.0396,0.1002]  \t[-0.0588,-0.0314]\t[-0.0320,-0.0115]\t[-0.0706,0.0017] \t[-0.0701,0.0707] \t[0.1089,0.1944]  \t[0.1402,0.3598]  \t[0.2686,0.4744]  \t[0.3199,0.8051]  \t0.60591   \n",
      "sampler.palgo = tensor([0.2812, 0.2401, 0.2095, 0.2692])\n",
      "9    \t[0.0464,0.0732]  \t[0.0185,0.0351]  \t[-0.0198,0.0006] \t[0.0243,0.0467]  \t[-0.0418,0.0119] \t[-0.0220,0.0501] \t[0.1076,0.1791]  \t[0.0859,0.1607]  \t[0.3416,0.4931]  \t[0.3453,0.5994]  \t0.83851   \n",
      "sampler.palgo = tensor([0.2896, 0.2497, 0.2014, 0.2593])\n",
      "17   \t[0.0284,0.0443]  \t[0.0074,0.0120]  \t[0.0130,0.0283]  \t[0.0399,0.0630]  \t[-0.0055,0.0330] \t[0.0310,0.0663]  \t[0.2205,0.3046]  \t[0.2910,0.4689]  \t[0.3023,0.4254]  \t[0.2382,0.3828]  \t1.08416   \n",
      "sampler.palgo = tensor([0.2906, 0.2390, 0.1961, 0.2744])\n",
      "33   \t[0.0178,0.0267]  \t[0.0051,0.0092]  \t[0.0289,0.0389]  \t[0.0422,0.0567]  \t[0.0160,0.0418]  \t[0.0284,0.0541]  \t[0.3598,0.4566]  \t[0.5089,0.6686]  \t[0.2760,0.3533]  \t[0.2187,0.3028]  \t1.33791   \n",
      "sampler.palgo = tensor([0.2790, 0.2226, 0.2039, 0.2945])\n",
      "65   \t[0.0125,0.0173]  \t[0.0064,0.0080]  \t[0.0434,0.0521]  \t[0.0540,0.0704]  \t[0.0311,0.0482]  \t[0.0388,0.0587]  \t[0.3322,0.3892]  \t[0.2741,0.3541]  \t[0.2842,0.3363]  \t[0.2704,0.3501]  \t1.65991   \n",
      "sampler.palgo = tensor([0.2381, 0.2116, 0.1938, 0.3565])\n",
      "129  \t[0.0102,0.0127]  \t[0.0072,0.0085]  \t[0.0603,0.0665]  \t[0.0728,0.0844]  \t[0.0468,0.0601]  \t[0.0581,0.0749]  \t[0.2667,0.3025]  \t[0.1919,0.2279]  \t[0.2517,0.2823]  \t[0.2069,0.2408]  \t2.20277   \n",
      "sampler.palgo = tensor([0.1089, 0.2357, 0.2245, 0.4310])\n",
      "257  \t[0.0092,0.0104]  \t[0.0076,0.0085]  \t[0.0728,0.0787]  \t[0.0823,0.0931]  \t[0.0634,0.0718]  \t[0.0767,0.0883]  \t[0.2105,0.2287]  \t[0.1406,0.1654]  \t[0.2026,0.2213]  \t[0.1404,0.1624]  \t3.45543   \n",
      "sampler.palgo = tensor([0.0443, 0.1918, 0.2049, 0.5590])\n",
      "513  \t[0.0075,0.0082]  \t[0.0056,0.0060]  \t[0.0815,0.0860]  \t[0.0892,0.0959]  \t[0.0741,0.0805]  \t[0.0823,0.0927]  \t[0.1602,0.1717]  \t[0.1074,0.1183]  \t[0.1639,0.1749]  \t[0.1230,0.1332]  \t5.00107   \n",
      "sampler.palgo = tensor([0.0277, 0.1303, 0.3258, 0.5161])\n",
      "1025 \t[0.0049,0.0052]  \t[0.0022,0.0024]  \t[0.0942,0.0969]  \t[0.1044,0.1097]  \t[0.0884,0.0927]  \t[0.1002,0.1062]  \t[0.1685,0.1745]  \t[0.1709,0.1828]  \t[0.1335,0.1393]  \t[0.0981,0.1054]  \t7.76941   \n",
      "sampler.palgo = tensor([0.0115, 0.0824, 0.6110, 0.2951])\n",
      "2049 \t[0.0029,0.0030]  \t[0.0009,0.0009]  \t[0.1018,0.1042]  \t[0.1088,0.1123]  \t[0.0977,0.1009]  \t[0.1063,0.1100]  \t[0.1895,0.1961]  \t[0.2114,0.2189]  \t[0.1041,0.1072]  \t[0.0732,0.0771]  \t13.80044  \n",
      "sampler.palgo = tensor([0.0059, 0.0516, 0.7961, 0.1464])\n",
      "2173 \t[0.0027,0.0029]  \t[0.0006,0.0007]  \t[0.1027,0.1054]  \t[0.1146,0.1264]  \t[0.0990,0.1018]  \t[0.1122,0.1249]  \t[0.1892,0.1954]  \t[0.1756,0.1946]  \t[0.1026,0.1053]  \t[0.0674,0.0780]  \t14.79990  \n",
      "sampler.palgo = tensor([0.0054, 0.0460, 0.7871, 0.1614])\n",
      "0.2: [0.04187652 0.0442152 ] 0.5: [0.09885925 0.1022912 ]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-biking-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.035973578465496724, 3.187013211955139, 0.8712958132211441, 8, 4096, 4\n",
    "    cost, price = 1/3, 1\n",
    "    \n",
    "    return learnOnline(mydata, seed=4545, q=0.5, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, cost=cost, price=price,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', cost=cost, price=price, approxargmax=True))\n",
    "\n",
    "neutralpunders = doit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f792b58",
   "metadata": {},
   "source": [
    "### risk-averse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8e089b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n    \tloss             \tsince last       \trahat            \tsince last       \tevarq(0.2)       \tsince last       \tpunder           \tsince last       \taccept           \tsince last       \tdt (sec)  \n",
      "1    \t[0.0750,0.3000]  \t[0.0750,0.3000]  \t[-0.1104,-0.0276]\t[-0.1104,-0.0276]\t[-0.2052,-0.0892]\t[-0.2091,-0.0690]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.0469,0.1875]  \t[0.0469,0.1875]  \t0.14608   \n",
      "sampler.palgo = tensor([0.1687, 0.1660, 0.1643, 0.1643, 0.1666, 0.1700])\n",
      "2    \t[0.0874,0.2324]  \t[0.0449,0.1794]  \t[-0.0622,-0.0218]\t[-0.0168,-0.0042]\t[-0.1584,-0.0690]\t[-0.1470,-0.0039]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.0703,0.1797]  \t[0.0469,0.1875]  \t0.36072   \n",
      "sampler.palgo = tensor([0.1686, 0.1663, 0.1646, 0.1680, 0.1658, 0.1667])\n",
      "3    \t[0.1069,0.2266]  \t[0.0626,0.2502]  \t[-0.0795,-0.0399]\t[-0.1345,-0.0336]\t[-0.1606,-0.0900]\t[-0.1802,-0.0737]\t[0.0000,0.0000]  \t[0.0000,0.0000]  \t[0.1716,0.3654]  \t[0.1875,0.7500]  \t0.57590   \n",
      "sampler.palgo = tensor([0.1678, 0.1723, 0.1626, 0.1666, 0.1617, 0.1690])\n",
      "5    \t[0.1053,0.1808]  \t[0.0629,0.1626]  \t[-0.0570,-0.0305]\t[-0.0290,-0.0102]\t[-0.1497,-0.1033]\t[-0.1530,-0.0645]\t[0.0123,0.0375]  \t[0.0312,0.0941]  \t[0.1622,0.3005]  \t[0.1090,0.2656]  \t0.75595   \n",
      "sampler.palgo = tensor([0.1711, 0.1717, 0.1744, 0.1585, 0.1605, 0.1638])\n",
      "9    \t[0.0713,0.1118]  \t[0.0202,0.0418]  \t[-0.0180,0.0026] \t[0.0275,0.0520]  \t[-0.1119,-0.0733]\t[-0.0590,-0.0066]\t[0.0815,0.1374]  \t[0.1443,0.2734]  \t[0.1661,0.2523]  \t[0.1287,0.2461]  \t0.92238   \n",
      "sampler.palgo = tensor([0.1728, 0.1705, 0.1835, 0.1575, 0.1562, 0.1596])\n",
      "17   \t[0.0396,0.0612]  \t[0.0030,0.0046]  \t[0.0117,0.0269]  \t[0.0355,0.0564]  \t[-0.0729,-0.0289]\t[-0.0036,0.0234] \t[0.2218,0.3246]  \t[0.3490,0.5450]  \t[0.1551,0.2209]  \t[0.1189,0.1915]  \t1.13562   \n",
      "sampler.palgo = tensor([0.1699, 0.1695, 0.1834, 0.1563, 0.1629, 0.1579])\n",
      "33   \t[0.0209,0.0321]  \t[0.0008,0.0013]  \t[0.0199,0.0283]  \t[0.0259,0.0347]  \t[-0.0399,-0.0128]\t[0.0132,0.0216]  \t[0.4315,0.5389]  \t[0.6219,0.8355]  \t[0.1240,0.1678]  \t[0.0829,0.1211]  \t1.39662   \n",
      "sampler.palgo = tensor([0.1739, 0.1772, 0.1788, 0.1524, 0.1625, 0.1551])\n",
      "65   \t[0.0109,0.0166]  \t[0.0004,0.0006]  \t[0.0212,0.0264]  \t[0.0209,0.0264]  \t[-0.0176,-0.0022]\t[0.0069,0.0137]  \t[0.5445,0.6308]  \t[0.6175,0.7862]  \t[0.1283,0.1539]  \t[0.1184,0.1570]  \t1.74437   \n",
      "sampler.palgo = tensor([0.1765, 0.1754, 0.1721, 0.1501, 0.1652, 0.1607])\n",
      "129  \t[0.0058,0.0087]  \t[0.0007,0.0009]  \t[0.0233,0.0263]  \t[0.0242,0.0272]  \t[-0.0061,0.0024] \t[-0.0007,0.0095] \t[0.6109,0.6732]  \t[0.6428,0.7311]  \t[0.1309,0.1482]  \t[0.1281,0.1507]  \t2.48605   \n",
      "sampler.palgo = tensor([0.1608, 0.1946, 0.1851, 0.1401, 0.1607, 0.1588])\n",
      "257  \t[0.0034,0.0049]  \t[0.0009,0.0011]  \t[0.0266,0.0285]  \t[0.0283,0.0317]  \t[0.0011,0.0075]  \t[0.0082,0.0132]  \t[0.6389,0.6883]  \t[0.6374,0.7149]  \t[0.1251,0.1376]  \t[0.1079,0.1261]  \t3.41526   \n",
      "sampler.palgo = tensor([0.1323, 0.2226, 0.1809, 0.1424, 0.1561, 0.1656])\n",
      "513  \t[0.0024,0.0032]  \t[0.0015,0.0016]  \t[0.0361,0.0382]  \t[0.0452,0.0487]  \t[0.0095,0.0130]  \t[0.0150,0.0201]  \t[0.6266,0.6600]  \t[0.6041,0.6470]  \t[0.1260,0.1349]  \t[0.1234,0.1340]  \t4.86206   \n",
      "sampler.palgo = tensor([0.1517, 0.1735, 0.2179, 0.1467, 0.1793, 0.1310])\n",
      "1025 \t[0.0028,0.0032]  \t[0.0031,0.0033]  \t[0.0639,0.0659]  \t[0.0907,0.0952]  \t[0.0227,0.0257]  \t[0.0337,0.0379]  \t[0.4547,0.4708]  \t[0.2736,0.2897]  \t[0.1434,0.1506]  \t[0.1596,0.1681]  \t8.10127   \n",
      "sampler.palgo = tensor([0.1012, 0.1210, 0.2814, 0.1377, 0.1074, 0.2513])\n",
      "2049 \t[0.0026,0.0028]  \t[0.0023,0.0024]  \t[0.0801,0.0822]  \t[0.0959,0.0989]  \t[0.0325,0.0344]  \t[0.0409,0.0435]  \t[0.3192,0.3276]  \t[0.1797,0.1869]  \t[0.1301,0.1336]  \t[0.1142,0.1201]  \t13.68834  \n",
      "sampler.palgo = tensor([0.0248, 0.0338, 0.0892, 0.1034, 0.2739, 0.4748])\n",
      "2173 \t[0.0025,0.0027]  \t[0.0020,0.0022]  \t[0.0814,0.0836]  \t[0.1001,0.1106]  \t[0.0333,0.0354]  \t[0.0465,0.0556]  \t[0.3072,0.3151]  \t[0.1073,0.1209]  \t[0.1272,0.1307]  \t[0.0742,0.0873]  \t14.62561  \n",
      "sampler.palgo = tensor([0.0205, 0.0313, 0.0747, 0.0954, 0.1898, 0.5883])\n",
      "0.2: [0.03380039 0.0354573 ] 0.5: [0.07824295 0.08089891]\n"
     ]
    }
   ],
   "source": [
    "def doit():\n",
    "    # see tune-biking-expectile.*.res\n",
    "    initlr, tzero, eta, gammamin, gammamax, nalgos = 0.03956762604016887, 2.508523593579849, 0.7535176557078329, 32, 4096, 6\n",
    "    cost, price = 1/3, 1\n",
    "    \n",
    "    return learnOnline(mydata, seed=4545, q=0.2, initlr=initlr, tzero=tzero, batch_size=8, \n",
    "                eta=eta, gammamin=gammamin, gammamax=gammamax, nalgos=nalgos, cost=cost, price=price,\n",
    "                modelfactory=lambda x: CauchyTruncatedNormal(dobs=x.shape[1], numrff=1024, sigma=1/10, device='cpu', cost=cost, price=price, approxargmax=True))\n",
    "\n",
    "aversepunders = doit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6c0a87af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def flass():    \n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "    plt.style.use('seaborn-colorblind')\n",
    "    minbatch = 7\n",
    "    \n",
    "    plt.errorbar(x=[8*x[0] for x in neutralpunders if x[0] > minbatch ], \n",
    "                 y=[ 50*(x[1][0]+x[1][1]) for x in neutralpunders if x[0] > minbatch ], \n",
    "                 yerr=[ 50*(x[1][1]-x[1][0]) for x in neutralpunders if x[0] > minbatch ],\n",
    "                 label='risk-neutral ($q=0.5$)',\n",
    "                 linewidth=2)\n",
    "    plt.errorbar(x=[8*x[0] for x in aversepunders if x[0] > minbatch ], \n",
    "                 y=[ 50*(x[1][0]+x[1][1]) for x in aversepunders if x[0] > minbatch ], \n",
    "                 yerr=[ 50*(x[1][1]-x[1][0]) for x in aversepunders if x[0] > minbatch ],\n",
    "                 label='risk-averse ($q=0.2$)',\n",
    "                 linewidth=2)\n",
    "    plt.xscale('log')\n",
    "    plt.xlabel('$t$', fontsize=16)\n",
    "    plt.ylim([0, 100])\n",
    "    #plt.title('Cumulative Sell Out (%), DC Dataset', fontsize=14)\n",
    "    plt.legend(fontsize=16)\n",
    "    plt.xticks(fontsize=16)\n",
    "    plt.yticks(fontsize=16)\n",
    "    plt.setp(plt.gca().spines.values(), linewidth=2)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('dcsellout.pdf')  \n",
    "    \n",
    "flass()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7963ec6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
